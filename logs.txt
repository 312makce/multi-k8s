* 
* ==> Audit <==
* |---------|----------------------------------|----------|------------------------|---------|---------------------|---------------------|
| Command |               Args               | Profile  |          User          | Version |     Start Time      |      End Time       |
|---------|----------------------------------|----------|------------------------|---------|---------------------|---------------------|
| start   |                                  | minikube | DESKTOP-VDOSF3B\YRYSKA | v1.28.0 | 12 Dec 22 21:53 CST |                     |
| start   | --vm-driver=hyperv               | minikube | DESKTOP-VDOSF3B\YRYSKA | v1.28.0 | 12 Dec 22 21:58 CST |                     |
|         | --memory=4096 --cpus=4           |          |                        |         |                     |                     |
|         | --hyperv-virtual-switch=External |          |                        |         |                     |                     |
|         | --v=7 --alsologtostderr          |          |                        |         |                     |                     |
| profile | list                             | minikube | DESKTOP-VDOSF3B\YRYSKA | v1.28.0 | 12 Dec 22 21:59 CST | 12 Dec 22 21:59 CST |
| start   |                                  | minikube | DESKTOP-VDOSF3B\YRYSKA | v1.28.0 | 12 Dec 22 21:59 CST | 12 Dec 22 22:01 CST |
| start   |                                  | minikube | DESKTOP-VDOSF3B\YRYSKA | v1.28.0 | 14 Dec 22 19:44 CST | 14 Dec 22 19:44 CST |
| addons  | enable ingress                   | minikube | DESKTOP-VDOSF3B\YRYSKA | v1.28.0 | 14 Dec 22 21:52 CST |                     |
| addons  | enable ingress                   | minikube | DESKTOP-VDOSF3B\YRYSKA | v1.28.0 | 14 Dec 22 22:00 CST |                     |
|---------|----------------------------------|----------|------------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/12/14 19:44:13
Running on machine: DESKTOP-VDOSF3B
Binary: Built with gc go1.19.2 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1214 19:44:13.388771   14256 out.go:296] Setting OutFile to fd 104 ...
I1214 19:44:13.388771   14256 out.go:348] isatty.IsTerminal(104) = true
I1214 19:44:13.388771   14256 out.go:309] Setting ErrFile to fd 108...
I1214 19:44:13.388771   14256 out.go:348] isatty.IsTerminal(108) = true
W1214 19:44:13.406789   14256 root.go:311] Error reading config file at C:\Users\YRYSKA\.minikube\config\config.json: open C:\Users\YRYSKA\.minikube\config\config.json: The system cannot find the file specified.
I1214 19:44:13.416720   14256 out.go:303] Setting JSON to false
I1214 19:44:13.432440   14256 start.go:116] hostinfo: {"hostname":"DESKTOP-VDOSF3B","uptime":1080468,"bootTime":1669988185,"procs":258,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621 Build 22621","kernelVersion":"10.0.22621 Build 22621","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"0af1c7bb-8e00-4747-87a9-00edadec167b"}
W1214 19:44:13.432440   14256 start.go:124] gopshost.Virtualization returned error: not implemented yet
I1214 19:44:13.433483   14256 out.go:177] üòÑ  minikube v1.28.0 on Microsoft Windows 11 Home 10.0.22621 Build 22621
I1214 19:44:13.434562   14256 notify.go:220] Checking for updates...
I1214 19:44:13.435099   14256 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1214 19:44:13.435099   14256 driver.go:365] Setting default libvirt URI to qemu:///system
I1214 19:44:13.602157   14256 docker.go:137] docker version: linux-20.10.21
I1214 19:44:13.604843   14256 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1214 19:44:14.251405   14256 info.go:266] docker info: {ID:YCLJ:O2RQ:2YLQ:RL35:BOI5:E3SZ:X5DL:BW73:3TXQ:U3UT:J3RC:YJKY Containers:58 ContainersRunning:45 ContainersPaused:0 ContainersStopped:13 Images:37 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:195 OomKillDisable:true NGoroutines:165 SystemTime:2022-12-15 01:44:13.7195805 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:6 KernelVersion:5.4.72-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3789733888 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:770bd0108c32f3fb5c73ae1264f7e503fe7b2661 Expected:770bd0108c32f3fb5c73ae1264f7e503fe7b2661} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.13.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.16] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.22.0]] Warnings:<nil>}}
I1214 19:44:14.253033   14256 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1214 19:44:14.253569   14256 start.go:282] selected driver: docker
I1214 19:44:14.253569   14256 start.go:808] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\YRYSKA:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1214 19:44:14.253569   14256 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1214 19:44:14.257805   14256 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1214 19:44:14.758907   14256 info.go:266] docker info: {ID:YCLJ:O2RQ:2YLQ:RL35:BOI5:E3SZ:X5DL:BW73:3TXQ:U3UT:J3RC:YJKY Containers:58 ContainersRunning:45 ContainersPaused:0 ContainersStopped:13 Images:37 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:195 OomKillDisable:true NGoroutines:165 SystemTime:2022-12-15 01:44:14.3804668 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:6 KernelVersion:5.4.72-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3789733888 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:770bd0108c32f3fb5c73ae1264f7e503fe7b2661 Expected:770bd0108c32f3fb5c73ae1264f7e503fe7b2661} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.13.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.16] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.22.0]] Warnings:<nil>}}
I1214 19:44:14.850593   14256 cni.go:95] Creating CNI manager for ""
I1214 19:44:14.850593   14256 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1214 19:44:14.851620   14256 start_flags.go:317] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\YRYSKA:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1214 19:44:14.852250   14256 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1214 19:44:14.852762   14256 cache.go:120] Beginning downloading kic base image for docker with docker
I1214 19:44:14.853293   14256 out.go:177] üöú  Pulling base image ...
I1214 19:44:14.853843   14256 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1214 19:44:14.853843   14256 image.go:76] Checking for gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 in local docker daemon
I1214 19:44:14.854391   14256 preload.go:148] Found local preload: C:\Users\YRYSKA\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4
I1214 19:44:14.854391   14256 cache.go:57] Caching tarball of preloaded images
I1214 19:44:14.854391   14256 preload.go:174] Found C:\Users\YRYSKA\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1214 19:44:14.854391   14256 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.3 on docker
I1214 19:44:14.854920   14256 profile.go:148] Saving config to C:\Users\YRYSKA\.minikube\profiles\minikube\config.json ...
I1214 19:44:15.019714   14256 image.go:80] Found gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 in local docker daemon, skipping pull
I1214 19:44:15.019714   14256 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 exists in daemon, skipping load
I1214 19:44:15.019714   14256 cache.go:208] Successfully downloaded all kic artifacts
I1214 19:44:15.019714   14256 start.go:364] acquiring machines lock for minikube: {Name:mkddb788873ec13f5fd327d8f52f7930bfe38654 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1214 19:44:15.019714   14256 start.go:368] acquired machines lock for "minikube" in 0s
I1214 19:44:15.020722   14256 start.go:96] Skipping create...Using existing machine configuration
I1214 19:44:15.020722   14256 fix.go:55] fixHost starting: 
I1214 19:44:15.024475   14256 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 19:44:15.174795   14256 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1214 19:44:15.174795   14256 fix.go:129] unexpected machine state, will restart: <nil>
I1214 19:44:15.175318   14256 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1214 19:44:15.177999   14256 cli_runner.go:164] Run: docker start minikube
I1214 19:44:15.762000   14256 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 19:44:15.915572   14256 kic.go:415] container "minikube" state is running.
I1214 19:44:15.918864   14256 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1214 19:44:16.084591   14256 profile.go:148] Saving config to C:\Users\YRYSKA\.minikube\profiles\minikube\config.json ...
I1214 19:44:16.086174   14256 machine.go:88] provisioning docker machine ...
I1214 19:44:16.086174   14256 ubuntu.go:169] provisioning hostname "minikube"
I1214 19:44:16.087767   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:16.256618   14256 main.go:134] libmachine: Using SSH client type: native
I1214 19:44:16.258370   14256 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x111bd60] 0x111ece0 <nil>  [] 0s} 127.0.0.1 63403 <nil> <nil>}
I1214 19:44:16.258370   14256 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1214 19:44:16.281384   14256 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1214 19:44:19.498105   14256 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1214 19:44:19.499766   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:19.663224   14256 main.go:134] libmachine: Using SSH client type: native
I1214 19:44:19.663224   14256 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x111bd60] 0x111ece0 <nil>  [] 0s} 127.0.0.1 63403 <nil> <nil>}
I1214 19:44:19.663224   14256 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1214 19:44:19.812774   14256 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1214 19:44:19.812774   14256 ubuntu.go:175] set auth options {CertDir:C:\Users\YRYSKA\.minikube CaCertPath:C:\Users\YRYSKA\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\YRYSKA\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\YRYSKA\.minikube\machines\server.pem ServerKeyPath:C:\Users\YRYSKA\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\YRYSKA\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\YRYSKA\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\YRYSKA\.minikube}
I1214 19:44:19.812774   14256 ubuntu.go:177] setting up certificates
I1214 19:44:19.812774   14256 provision.go:83] configureAuth start
I1214 19:44:19.814378   14256 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1214 19:44:20.001285   14256 provision.go:138] copyHostCerts
I1214 19:44:20.009926   14256 exec_runner.go:144] found C:\Users\YRYSKA\.minikube/key.pem, removing ...
I1214 19:44:20.009926   14256 exec_runner.go:207] rm: C:\Users\YRYSKA\.minikube\key.pem
I1214 19:44:20.009926   14256 exec_runner.go:151] cp: C:\Users\YRYSKA\.minikube\certs\key.pem --> C:\Users\YRYSKA\.minikube/key.pem (1675 bytes)
I1214 19:44:20.015940   14256 exec_runner.go:144] found C:\Users\YRYSKA\.minikube/ca.pem, removing ...
I1214 19:44:20.015940   14256 exec_runner.go:207] rm: C:\Users\YRYSKA\.minikube\ca.pem
I1214 19:44:20.016515   14256 exec_runner.go:151] cp: C:\Users\YRYSKA\.minikube\certs\ca.pem --> C:\Users\YRYSKA\.minikube/ca.pem (1078 bytes)
I1214 19:44:20.022903   14256 exec_runner.go:144] found C:\Users\YRYSKA\.minikube/cert.pem, removing ...
I1214 19:44:20.022903   14256 exec_runner.go:207] rm: C:\Users\YRYSKA\.minikube\cert.pem
I1214 19:44:20.022903   14256 exec_runner.go:151] cp: C:\Users\YRYSKA\.minikube\certs\cert.pem --> C:\Users\YRYSKA\.minikube/cert.pem (1119 bytes)
I1214 19:44:20.023434   14256 provision.go:112] generating server cert: C:\Users\YRYSKA\.minikube\machines\server.pem ca-key=C:\Users\YRYSKA\.minikube\certs\ca.pem private-key=C:\Users\YRYSKA\.minikube\certs\ca-key.pem org=YRYSKA.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1214 19:44:20.068594   14256 provision.go:172] copyRemoteCerts
I1214 19:44:20.073269   14256 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1214 19:44:20.073269   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:20.239576   14256 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63403 SSHKeyPath:C:\Users\YRYSKA\.minikube\machines\minikube\id_rsa Username:docker}
I1214 19:44:20.297380   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I1214 19:44:20.327539   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1214 19:44:20.350479   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1214 19:44:20.373958   14256 provision.go:86] duration metric: configureAuth took 561.1849ms
I1214 19:44:20.373958   14256 ubuntu.go:193] setting minikube options for container-runtime
I1214 19:44:20.374496   14256 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1214 19:44:20.376705   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:20.536449   14256 main.go:134] libmachine: Using SSH client type: native
I1214 19:44:20.536993   14256 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x111bd60] 0x111ece0 <nil>  [] 0s} 127.0.0.1 63403 <nil> <nil>}
I1214 19:44:20.536993   14256 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1214 19:44:20.705787   14256 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1214 19:44:20.706334   14256 ubuntu.go:71] root file system type: overlay
I1214 19:44:20.706334   14256 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1214 19:44:20.708490   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:20.867922   14256 main.go:134] libmachine: Using SSH client type: native
I1214 19:44:20.867974   14256 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x111bd60] 0x111ece0 <nil>  [] 0s} 127.0.0.1 63403 <nil> <nil>}
I1214 19:44:20.867974   14256 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1214 19:44:21.022551   14256 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1214 19:44:21.025253   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:21.183594   14256 main.go:134] libmachine: Using SSH client type: native
I1214 19:44:21.184263   14256 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x111bd60] 0x111ece0 <nil>  [] 0s} 127.0.0.1 63403 <nil> <nil>}
I1214 19:44:21.184263   14256 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1214 19:44:21.278637   14256 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1214 19:44:21.278637   14256 machine.go:91] provisioned docker machine in 5.1924631s
I1214 19:44:21.278637   14256 start.go:300] post-start starting for "minikube" (driver="docker")
I1214 19:44:21.278637   14256 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1214 19:44:21.281313   14256 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1214 19:44:21.283486   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:21.444317   14256 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63403 SSHKeyPath:C:\Users\YRYSKA\.minikube\machines\minikube\id_rsa Username:docker}
I1214 19:44:21.507431   14256 ssh_runner.go:195] Run: cat /etc/os-release
I1214 19:44:21.512737   14256 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1214 19:44:21.512737   14256 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1214 19:44:21.512737   14256 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1214 19:44:21.512737   14256 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1214 19:44:21.512737   14256 filesync.go:126] Scanning C:\Users\YRYSKA\.minikube\addons for local assets ...
I1214 19:44:21.513267   14256 filesync.go:126] Scanning C:\Users\YRYSKA\.minikube\files for local assets ...
I1214 19:44:21.513267   14256 start.go:303] post-start completed in 234.6298ms
I1214 19:44:21.515941   14256 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1214 19:44:21.517545   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:21.678581   14256 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63403 SSHKeyPath:C:\Users\YRYSKA\.minikube\machines\minikube\id_rsa Username:docker}
I1214 19:44:21.743500   14256 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1214 19:44:21.750583   14256 fix.go:57] fixHost completed within 6.7298615s
I1214 19:44:21.750583   14256 start.go:83] releasing machines lock for "minikube", held for 6.7308691s
I1214 19:44:21.752711   14256 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1214 19:44:21.901487   14256 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1214 19:44:21.904179   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:21.904762   14256 ssh_runner.go:195] Run: systemctl --version
I1214 19:44:21.907465   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:22.069626   14256 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63403 SSHKeyPath:C:\Users\YRYSKA\.minikube\machines\minikube\id_rsa Username:docker}
I1214 19:44:22.084732   14256 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63403 SSHKeyPath:C:\Users\YRYSKA\.minikube\machines\minikube\id_rsa Username:docker}
I1214 19:44:22.589603   14256 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1214 19:44:22.603347   14256 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1214 19:44:22.606012   14256 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1214 19:44:22.622590   14256 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1214 19:44:22.640329   14256 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1214 19:44:22.716964   14256 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1214 19:44:22.788162   14256 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1214 19:44:22.849726   14256 ssh_runner.go:195] Run: sudo systemctl restart docker
I1214 19:44:23.205762   14256 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1214 19:44:23.265502   14256 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1214 19:44:23.321515   14256 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1214 19:44:23.334896   14256 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1214 19:44:23.337611   14256 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1214 19:44:23.344586   14256 start.go:472] Will wait 60s for crictl version
I1214 19:44:23.347302   14256 ssh_runner.go:195] Run: sudo crictl version
I1214 19:44:24.208352   14256 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.20
RuntimeApiVersion:  1.41.0
I1214 19:44:24.209992   14256 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1214 19:44:24.689195   14256 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1214 19:44:24.724388   14256 out.go:204] üê≥  Preparing Kubernetes v1.25.3 on Docker 20.10.20 ...
I1214 19:44:24.726536   14256 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1214 19:44:24.995994   14256 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1214 19:44:25.000789   14256 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1214 19:44:25.008978   14256 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1214 19:44:25.029471   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1214 19:44:25.180243   14256 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1214 19:44:25.182988   14256 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1214 19:44:25.244163   14256 docker.go:613] Got preloaded images: -- stdout --
redis:latest
postgres:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
stephengrider/multi-worker:latest
stephengrider/multi-server:latest
stephengrider/multi-client:latest

-- /stdout --
I1214 19:44:25.244163   14256 docker.go:543] Images already preloaded, skipping extraction
I1214 19:44:25.246382   14256 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1214 19:44:25.276439   14256 docker.go:613] Got preloaded images: -- stdout --
redis:latest
postgres:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
stephengrider/multi-worker:latest
stephengrider/multi-server:latest
stephengrider/multi-client:latest

-- /stdout --
I1214 19:44:25.276439   14256 cache_images.go:84] Images are preloaded, skipping loading
I1214 19:44:25.278043   14256 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1214 19:44:26.088943   14256 cni.go:95] Creating CNI manager for ""
I1214 19:44:26.088943   14256 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1214 19:44:26.088943   14256 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1214 19:44:26.088943   14256 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I1214 19:44:26.088943   14256 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1214 19:44:26.088943   14256 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1214 19:44:26.092119   14256 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.3
I1214 19:44:26.106213   14256 binaries.go:44] Found k8s binaries, skipping transfer
I1214 19:44:26.109422   14256 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1214 19:44:26.119649   14256 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1214 19:44:26.137600   14256 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1214 19:44:26.153981   14256 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2031 bytes)
I1214 19:44:26.174554   14256 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1214 19:44:26.179460   14256 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1214 19:44:26.191292   14256 certs.go:54] Setting up C:\Users\YRYSKA\.minikube\profiles\minikube for IP: 192.168.49.2
I1214 19:44:26.197264   14256 certs.go:182] skipping minikubeCA CA generation: C:\Users\YRYSKA\.minikube\ca.key
I1214 19:44:26.207463   14256 certs.go:182] skipping proxyClientCA CA generation: C:\Users\YRYSKA\.minikube\proxy-client-ca.key
I1214 19:44:26.208058   14256 certs.go:298] skipping minikube-user signed cert generation: C:\Users\YRYSKA\.minikube\profiles\minikube\client.key
I1214 19:44:26.217790   14256 certs.go:298] skipping minikube signed cert generation: C:\Users\YRYSKA\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1214 19:44:26.228843   14256 certs.go:298] skipping aggregator signed cert generation: C:\Users\YRYSKA\.minikube\profiles\minikube\proxy-client.key
I1214 19:44:26.232014   14256 certs.go:388] found cert: C:\Users\YRYSKA\.minikube\certs\C:\Users\YRYSKA\.minikube\certs\ca-key.pem (1675 bytes)
I1214 19:44:26.232520   14256 certs.go:388] found cert: C:\Users\YRYSKA\.minikube\certs\C:\Users\YRYSKA\.minikube\certs\ca.pem (1078 bytes)
I1214 19:44:26.232591   14256 certs.go:388] found cert: C:\Users\YRYSKA\.minikube\certs\C:\Users\YRYSKA\.minikube\certs\cert.pem (1119 bytes)
I1214 19:44:26.232591   14256 certs.go:388] found cert: C:\Users\YRYSKA\.minikube\certs\C:\Users\YRYSKA\.minikube\certs\key.pem (1675 bytes)
I1214 19:44:26.233098   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1214 19:44:26.257406   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1214 19:44:26.279217   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1214 19:44:26.303102   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1214 19:44:26.323894   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1214 19:44:26.347618   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1214 19:44:26.370349   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1214 19:44:26.392485   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1214 19:44:26.419837   14256 ssh_runner.go:362] scp C:\Users\YRYSKA\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1214 19:44:26.441187   14256 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1214 19:44:26.459639   14256 ssh_runner.go:195] Run: openssl version
I1214 19:44:26.475518   14256 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1214 19:44:26.489435   14256 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1214 19:44:26.494811   14256 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Dec 13 03:56 /usr/share/ca-certificates/minikubeCA.pem
I1214 19:44:26.497483   14256 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1214 19:44:26.509237   14256 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1214 19:44:26.520360   14256 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\YRYSKA:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1214 19:44:26.521926   14256 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1214 19:44:26.555814   14256 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1214 19:44:26.566053   14256 kubeadm.go:411] found existing configuration files, will attempt cluster restart
I1214 19:44:26.566053   14256 kubeadm.go:627] restartCluster start
I1214 19:44:26.568921   14256 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1214 19:44:26.579675   14256 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1214 19:44:26.581239   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1214 19:44:26.732436   14256 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:57884"
I1214 19:44:26.732436   14256 kubeconfig.go:135] verify returned: got: 127.0.0.1:57884, want: 127.0.0.1:63402
I1214 19:44:26.735097   14256 lock.go:35] WriteFile acquiring C:\Users\YRYSKA\.kube\config: {Name:mk10e07a36024f5ee59f22c55c4df151fda290dd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1214 19:44:26.753031   14256 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1214 19:44:26.766274   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:26.768932   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:26.783201   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:26.991587   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:26.996268   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:27.010018   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:27.191396   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:27.194767   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:27.208241   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:27.392223   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:27.395918   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:27.407358   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:27.593269   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:27.595902   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:27.609016   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:27.793488   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:27.796895   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:27.809327   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:27.995790   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:27.999141   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:28.011475   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:28.198108   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:28.202001   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:28.213010   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:28.390776   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:28.394552   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:28.405901   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:28.593080   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:28.596996   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:28.607868   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:28.793676   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:28.796889   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:28.808370   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:28.993424   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:28.997242   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:29.009332   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:29.194881   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:29.198909   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:29.211383   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:29.395795   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:29.399003   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:29.410245   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:29.596621   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:29.599002   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:29.609805   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:29.797054   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:29.801172   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:29.812685   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:29.812685   14256 api_server.go:165] Checking apiserver status ...
I1214 19:44:29.815489   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 19:44:29.826400   14256 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1214 19:44:29.826400   14256 kubeadm.go:602] needs reconfigure: apiserver error: timed out waiting for the condition
I1214 19:44:29.826400   14256 kubeadm.go:1114] stopping kube-system containers ...
I1214 19:44:29.828556   14256 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1214 19:44:29.859814   14256 docker.go:444] Stopping containers: [a4053bd6969a 11259ef04cfc f3aac486d7fc d3880198668d 1ee865686ea9 40f8a17db9d1 a139154a1429 26929076817d 7a2d092ce8de fd2082751d7c 1d1cab09d01d 8e1863f477fa c1a62bf252ce b0bf9ec3d89e 9d7893a6de44]
I1214 19:44:29.861960   14256 ssh_runner.go:195] Run: docker stop a4053bd6969a 11259ef04cfc f3aac486d7fc d3880198668d 1ee865686ea9 40f8a17db9d1 a139154a1429 26929076817d 7a2d092ce8de fd2082751d7c 1d1cab09d01d 8e1863f477fa c1a62bf252ce b0bf9ec3d89e 9d7893a6de44
I1214 19:44:29.893129   14256 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1214 19:44:29.908666   14256 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1214 19:44:29.919393   14256 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Dec 13 04:01 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Dec 13 04:01 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Dec 13 04:01 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Dec 13 04:01 /etc/kubernetes/scheduler.conf

I1214 19:44:29.922051   14256 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1214 19:44:29.936474   14256 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1214 19:44:29.950065   14256 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1214 19:44:29.962468   14256 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1214 19:44:29.965159   14256 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1214 19:44:29.977939   14256 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1214 19:44:29.987600   14256 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1214 19:44:29.990295   14256 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1214 19:44:30.002740   14256 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1214 19:44:30.012424   14256 kubeadm.go:704] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1214 19:44:30.012424   14256 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1214 19:44:30.340494   14256 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1214 19:44:30.838833   14256 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1214 19:44:30.970335   14256 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1214 19:44:31.062021   14256 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1214 19:44:31.122613   14256 api_server.go:51] waiting for apiserver process to appear ...
I1214 19:44:31.125794   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:31.646001   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:32.143022   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:32.664538   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:33.146502   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:33.644084   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:34.143415   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:34.645013   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:34.662658   14256 api_server.go:71] duration metric: took 3.5395322s to wait for apiserver process to appear ...
I1214 19:44:34.662658   14256 api_server.go:87] waiting for apiserver healthz status ...
I1214 19:44:34.662713   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:34.666514   14256 api_server.go:268] stopped: https://127.0.0.1:63402/healthz: Get "https://127.0.0.1:63402/healthz": EOF
I1214 19:44:35.174303   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:35.177751   14256 api_server.go:268] stopped: https://127.0.0.1:63402/healthz: Get "https://127.0.0.1:63402/healthz": EOF
I1214 19:44:35.671559   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:35.673883   14256 api_server.go:268] stopped: https://127.0.0.1:63402/healthz: Get "https://127.0.0.1:63402/healthz": EOF
I1214 19:44:36.175935   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:36.178715   14256 api_server.go:268] stopped: https://127.0.0.1:63402/healthz: Get "https://127.0.0.1:63402/healthz": EOF
I1214 19:44:36.675329   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:41.100473   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1214 19:44:41.103854   14256 api_server.go:102] status: https://127.0.0.1:63402/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1214 19:44:41.179721   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:41.206366   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1214 19:44:41.206878   14256 api_server.go:102] status: https://127.0.0.1:63402/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1214 19:44:41.670450   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:41.681315   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 19:44:41.681315   14256 api_server.go:102] status: https://127.0.0.1:63402/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 19:44:42.172970   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:42.203174   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 19:44:42.203716   14256 api_server.go:102] status: https://127.0.0.1:63402/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 19:44:42.670540   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:42.705883   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 19:44:42.705883   14256 api_server.go:102] status: https://127.0.0.1:63402/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 19:44:43.169690   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:43.300491   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 19:44:43.300491   14256 api_server.go:102] status: https://127.0.0.1:63402/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 19:44:43.672593   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:43.705554   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 19:44:43.705554   14256 api_server.go:102] status: https://127.0.0.1:63402/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 19:44:44.167488   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:44.205227   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 200:
ok
I1214 19:44:44.315882   14256 api_server.go:140] control plane version: v1.25.3
I1214 19:44:44.315882   14256 api_server.go:130] duration metric: took 9.6532241s to wait for apiserver health ...
I1214 19:44:44.315882   14256 cni.go:95] Creating CNI manager for ""
I1214 19:44:44.315882   14256 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1214 19:44:44.316426   14256 system_pods.go:43] waiting for kube-system pods to appear ...
I1214 19:44:44.443812   14256 system_pods.go:59] 7 kube-system pods found
I1214 19:44:44.443812   14256 system_pods.go:61] "coredns-565d847f94-jhwc8" [2fd59e94-c009-4d47-81f8-81104a0421b9] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1214 19:44:44.444354   14256 system_pods.go:61] "etcd-minikube" [a7c72b15-da3d-4511-a955-17352ae9b0ed] Running
I1214 19:44:44.444354   14256 system_pods.go:61] "kube-apiserver-minikube" [a6a57709-53de-484d-b04f-cc81cbf1003c] Running
I1214 19:44:44.444354   14256 system_pods.go:61] "kube-controller-manager-minikube" [85324974-ea75-45ac-b345-85ad64a5dc13] Running
I1214 19:44:44.444354   14256 system_pods.go:61] "kube-proxy-pmkhg" [5fe8de23-f5a8-42e9-9006-fffc9bdf2067] Running
I1214 19:44:44.444354   14256 system_pods.go:61] "kube-scheduler-minikube" [0c526b77-51d1-4807-8d7e-2035f397ce0b] Running
I1214 19:44:44.444354   14256 system_pods.go:61] "storage-provisioner" [2166a2f0-92cc-41ae-b995-e88891023ebb] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1214 19:44:44.444354   14256 system_pods.go:74] duration metric: took 127.9283ms to wait for pod list to return data ...
I1214 19:44:44.444354   14256 node_conditions.go:102] verifying NodePressure condition ...
I1214 19:44:44.511143   14256 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I1214 19:44:44.511143   14256 node_conditions.go:123] node cpu capacity is 12
I1214 19:44:44.511674   14256 node_conditions.go:105] duration metric: took 67.32ms to run NodePressure ...
I1214 19:44:44.512271   14256 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1214 19:44:48.499164   14256 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (3.9868921s)
I1214 19:44:48.499164   14256 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1214 19:44:48.705080   14256 ops.go:34] apiserver oom_adj: -16
I1214 19:44:48.705080   14256 kubeadm.go:631] restartCluster took 22.1390273s
I1214 19:44:48.705080   14256 kubeadm.go:398] StartCluster complete in 22.1847205s
I1214 19:44:48.705139   14256 settings.go:142] acquiring lock: {Name:mk30a8e4a8c41aba604972ff5917494b2eabe829 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1214 19:44:48.705139   14256 settings.go:150] Updating kubeconfig:  C:\Users\YRYSKA\.kube\config
I1214 19:44:48.707608   14256 lock.go:35] WriteFile acquiring C:\Users\YRYSKA\.kube\config: {Name:mk10e07a36024f5ee59f22c55c4df151fda290dd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1214 19:44:48.809004   14256 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1214 19:44:48.810086   14256 start.go:212] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1214 19:44:48.810629   14256 out.go:177] üîé  Verifying Kubernetes components...
I1214 19:44:48.810629   14256 addons.go:486] enableAddons start: toEnable=map[default-storageclass:true storage-provisioner:true], additional=[]
I1214 19:44:48.812241   14256 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1214 19:44:48.812241   14256 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1214 19:44:48.812241   14256 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1214 19:44:48.812241   14256 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1214 19:44:48.812241   14256 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1214 19:44:48.812757   14256 addons.go:227] Setting addon storage-provisioner=true in "minikube"
W1214 19:44:48.813860   14256 addons.go:236] addon storage-provisioner should already be in state true
I1214 19:44:48.814919   14256 host.go:66] Checking if "minikube" exists ...
I1214 19:44:48.818749   14256 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1214 19:44:48.820863   14256 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 19:44:48.820863   14256 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 19:44:49.063197   14256 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1214 19:44:49.064275   14256 addons.go:419] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1214 19:44:49.064275   14256 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1214 19:44:49.066769   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:49.268982   14256 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63403 SSHKeyPath:C:\Users\YRYSKA\.minikube\machines\minikube\id_rsa Username:docker}
I1214 19:44:49.306327   14256 addons.go:227] Setting addon default-storageclass=true in "minikube"
W1214 19:44:49.306327   14256 addons.go:236] addon default-storageclass should already be in state true
I1214 19:44:49.306327   14256 host.go:66] Checking if "minikube" exists ...
I1214 19:44:49.316306   14256 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 19:44:49.535570   14256 addons.go:419] installing /etc/kubernetes/addons/storageclass.yaml
I1214 19:44:49.535793   14256 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1214 19:44:49.539056   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 19:44:49.737192   14256 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63403 SSHKeyPath:C:\Users\YRYSKA\.minikube\machines\minikube\id_rsa Username:docker}
I1214 19:44:50.604984   14256 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1214 19:44:50.624044   14256 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1214 19:44:51.520010   14256 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (2.7012608s)
I1214 19:44:51.520010   14256 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (2.7077693s)
I1214 19:44:51.520010   14256 start.go:806] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1214 19:44:51.522170   14256 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1214 19:44:51.696064   14256 api_server.go:51] waiting for apiserver process to appear ...
I1214 19:44:51.704723   14256 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 19:44:55.827011   14256 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.2220266s)
I1214 19:44:55.827011   14256 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (5.2029666s)
I1214 19:44:55.827011   14256 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (4.1222874s)
I1214 19:44:55.827011   14256 api_server.go:71] duration metric: took 7.0169249s to wait for apiserver process to appear ...
I1214 19:44:55.827011   14256 api_server.go:87] waiting for apiserver healthz status ...
I1214 19:44:55.827011   14256 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:63402/healthz ...
I1214 19:44:55.829838   14256 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1214 19:44:55.831646   14256 addons.go:488] enableAddons completed in 7.0215597s
I1214 19:44:55.897209   14256 api_server.go:278] https://127.0.0.1:63402/healthz returned 200:
ok
I1214 19:44:55.901185   14256 api_server.go:140] control plane version: v1.25.3
I1214 19:44:55.901185   14256 api_server.go:130] duration metric: took 74.1743ms to wait for apiserver health ...
I1214 19:44:55.901185   14256 system_pods.go:43] waiting for kube-system pods to appear ...
I1214 19:44:55.923478   14256 system_pods.go:59] 7 kube-system pods found
I1214 19:44:55.923478   14256 system_pods.go:61] "coredns-565d847f94-jhwc8" [2fd59e94-c009-4d47-81f8-81104a0421b9] Running
I1214 19:44:55.923478   14256 system_pods.go:61] "etcd-minikube" [a7c72b15-da3d-4511-a955-17352ae9b0ed] Running
I1214 19:44:55.923478   14256 system_pods.go:61] "kube-apiserver-minikube" [a6a57709-53de-484d-b04f-cc81cbf1003c] Running
I1214 19:44:55.923478   14256 system_pods.go:61] "kube-controller-manager-minikube" [85324974-ea75-45ac-b345-85ad64a5dc13] Running
I1214 19:44:55.923478   14256 system_pods.go:61] "kube-proxy-pmkhg" [5fe8de23-f5a8-42e9-9006-fffc9bdf2067] Running
I1214 19:44:55.923478   14256 system_pods.go:61] "kube-scheduler-minikube" [0c526b77-51d1-4807-8d7e-2035f397ce0b] Running
I1214 19:44:55.923478   14256 system_pods.go:61] "storage-provisioner" [2166a2f0-92cc-41ae-b995-e88891023ebb] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1214 19:44:55.923478   14256 system_pods.go:74] duration metric: took 22.2931ms to wait for pod list to return data ...
I1214 19:44:55.923478   14256 kubeadm.go:573] duration metric: took 7.1133923s to wait for : map[apiserver:true system_pods:true] ...
I1214 19:44:55.923478   14256 node_conditions.go:102] verifying NodePressure condition ...
I1214 19:44:55.932012   14256 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I1214 19:44:55.932012   14256 node_conditions.go:123] node cpu capacity is 12
I1214 19:44:55.932012   14256 node_conditions.go:105] duration metric: took 8.5342ms to run NodePressure ...
I1214 19:44:55.932012   14256 start.go:217] waiting for startup goroutines ...
I1214 19:44:55.941427   14256 ssh_runner.go:195] Run: rm -f paused
I1214 19:44:56.148164   14256 start.go:506] kubectl: 1.25.2, cluster: 1.25.3 (minor skew: 0)
I1214 19:44:56.149292   14256 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Thu 2022-12-15 01:44:16 UTC, end at Thu 2022-12-15 04:04:32 UTC. --
Dec 15 02:28:48 minikube dockerd[1025]: time="2022-12-15T02:28:48.145220500Z" level=info msg="ignoring event" container=875d5a721db196812e4f1cd17cedeb5c44f8d6c22f917fa38747d7cfdedcf050 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:31:40 minikube dockerd[1025]: time="2022-12-15T02:31:40.153859100Z" level=info msg="ignoring event" container=97e29f26f93a74edaeb44ea4daa1060e0cc8ffa1c1a0545e3a2eba99f02c167f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:36:53 minikube dockerd[1025]: time="2022-12-15T02:36:53.090815800Z" level=info msg="ignoring event" container=d3f1a512956b29e1668d304b2e50447227eb29d8c6b292b0630ae47295719610 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:42:01 minikube dockerd[1025]: time="2022-12-15T02:42:01.072978400Z" level=info msg="ignoring event" container=e2defbccf53aa76e1da715f07f518dd298eb64c831abceba8c953b491cc6685c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:45:08 minikube dockerd[1025]: time="2022-12-15T02:45:08.285499900Z" level=info msg="ignoring event" container=91e28d07a4f1e8390910ca2cf9ed5ba5f2f7a9626a16c1bb63be2dadd6d06c8c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:45:10 minikube dockerd[1025]: time="2022-12-15T02:45:10.304219000Z" level=info msg="ignoring event" container=4ccb1c516ca5e0ea16888bf2f57a89a145aa9b7098f2e2c227413de2d8d15518 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:45:10 minikube dockerd[1025]: time="2022-12-15T02:45:10.875640900Z" level=info msg="ignoring event" container=0068bc8c43eee9afa1e649b0f067a249accbb4cf015bc12abf9c95aa502364ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:45:11 minikube dockerd[1025]: time="2022-12-15T02:45:11.907323000Z" level=info msg="ignoring event" container=06d9643dbaf6ef1cd3d3a346e99a23d13f81dd854ab2e1dfb35e628fda9869ad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:47:07 minikube dockerd[1025]: time="2022-12-15T02:47:07.141411000Z" level=info msg="ignoring event" container=cd96e09e0ac660beb39f7f32c42c569b951c462bd014fe02279aad9eb33386e3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:52:20 minikube dockerd[1025]: time="2022-12-15T02:52:20.384679000Z" level=info msg="ignoring event" container=765288107a059506e633539e4a54a50e216631c5afadb51af88ace36f6121d04 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:52:46 minikube dockerd[1025]: time="2022-12-15T02:52:46.488952700Z" level=info msg="ignoring event" container=1fb3edb2366c36794e4cba17eebff43ed44d501e86f277753ae0a3aa80060069 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:52:47 minikube dockerd[1025]: time="2022-12-15T02:52:47.604576400Z" level=info msg="ignoring event" container=8d9c836ef68e00143c3e544599563c403fdf9f7e13fc6534612598855c593243 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:53:05 minikube dockerd[1025]: time="2022-12-15T02:53:05.055736000Z" level=info msg="ignoring event" container=f0fb95baed16cfad63cde95f89120d04ddf385a8463f3651cc3564e1ad40693c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:53:31 minikube dockerd[1025]: time="2022-12-15T02:53:31.984517200Z" level=info msg="ignoring event" container=6c068abb68b4297d6e1b59a289252bfdbcfea0a727933e1ad5a029085ba10d7f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:22 minikube dockerd[1025]: time="2022-12-15T02:54:22.012408400Z" level=info msg="ignoring event" container=343ce32709ea160aa8d828dadb0f76378c1ef7adbfe55c47e67fb24c65905076 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:47 minikube dockerd[1025]: time="2022-12-15T02:54:47.349159700Z" level=info msg="ignoring event" container=79797b4eb079110e18409aa706252120f194130bdcdb203f614ed62ff3a0aaea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:47 minikube dockerd[1025]: time="2022-12-15T02:54:47.607267700Z" level=info msg="ignoring event" container=edd53cf2f4a882a35cb93149e9374da872721fae2d980255adec31fcc350063f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:50 minikube dockerd[1025]: time="2022-12-15T02:54:50.731133000Z" level=info msg="ignoring event" container=29d0f6bcb98052db1fb6a99a2fceebb560e0b6f2e129e87154a6985e5f3d9983 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:50 minikube dockerd[1025]: time="2022-12-15T02:54:50.942290000Z" level=info msg="ignoring event" container=fd1a1d854a654abf7ffa689d03a9ad53e52e10748527ec9de3ef756fee31c23b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:52 minikube dockerd[1025]: time="2022-12-15T02:54:52.228384000Z" level=info msg="ignoring event" container=a97562380b1fced4a8e4f9359c7d3fc00ef0fd53683adf3e680606285eb45766 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:52 minikube dockerd[1025]: time="2022-12-15T02:54:52.835142500Z" level=info msg="ignoring event" container=c943dcf2cbf71d6d2947e522195c9e23dc6ff7c0423b79c93d294d0a6b7508c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:55 minikube dockerd[1025]: time="2022-12-15T02:54:55.161092200Z" level=info msg="ignoring event" container=4b42354159a18c3c5963ebab33f5fe9d3db2bd456cdabeb7dfc5c11b33eec7e2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:54:55 minikube dockerd[1025]: time="2022-12-15T02:54:55.380324800Z" level=info msg="ignoring event" container=efc5329115012da39d2e103cadddbd51d054a99cb8e59e9b4d25d873b021e5c2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:55:51 minikube dockerd[1025]: time="2022-12-15T02:55:51.941512500Z" level=info msg="ignoring event" container=51534baf528bf9b0e0548482b9e3581108372a4c1052f3713ac8288def6874a4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:57:24 minikube dockerd[1025]: time="2022-12-15T02:57:24.995466900Z" level=info msg="ignoring event" container=02e7e6a9e4644042ede101173c64ca711941e27a820ea1f767a7f1afb29cd963 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 02:58:45 minikube dockerd[1025]: time="2022-12-15T02:58:45.975476000Z" level=info msg="ignoring event" container=fe50d8d260aa6e2c2ea4534df108b3659f58e3754de1696a9f1c45f25b5629c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:02:28 minikube dockerd[1025]: time="2022-12-15T03:02:28.999361100Z" level=info msg="ignoring event" container=5dcb302eb54ad08f3dcf23890d23e01c46f40c0c8236f4e7cf9a1bacb5b1569b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:04:00 minikube dockerd[1025]: time="2022-12-15T03:04:00.014401900Z" level=info msg="ignoring event" container=8ef80f6c0f20687314900357c3dc1cf3f7271c88e35a47f0d587bdcbc31d2790 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:07:34 minikube dockerd[1025]: time="2022-12-15T03:07:34.889962800Z" level=info msg="ignoring event" container=c52f584593626de75f95c615c64d4a131428865baf2745caea8b04bd08909df2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:09:06 minikube dockerd[1025]: time="2022-12-15T03:09:06.964194300Z" level=info msg="ignoring event" container=149de849c42b5d7cc3bf879bad2c7285125f2080865dc50d4c365fa3e4bb0a18 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:12:41 minikube dockerd[1025]: time="2022-12-15T03:12:41.032993700Z" level=info msg="ignoring event" container=791e828b51c3d0f19dba14ec1ed8081e6ac636930d7326af96682cdf7dd8517e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:14:16 minikube dockerd[1025]: time="2022-12-15T03:14:16.990217100Z" level=info msg="ignoring event" container=25a323a1fe32511069c012fad6bfd27e97c1408953a5d8d5ce606ab69fd16716 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:17:47 minikube dockerd[1025]: time="2022-12-15T03:17:47.998532200Z" level=info msg="ignoring event" container=9b968bf60c8e9e38cde9fbb35cddcc03c944e2487a591223bc89818dd7f4181d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:19:21 minikube dockerd[1025]: time="2022-12-15T03:19:21.880872300Z" level=info msg="ignoring event" container=8455f50c3f427d9a27d4694dd6c10053ad6c74309d37220aae9f8e2cb928aa62 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:22:58 minikube dockerd[1025]: time="2022-12-15T03:22:58.136549000Z" level=info msg="ignoring event" container=639cd9a07b09e85b4ef6b3c73b3d7b948647d0cc029853271a09a0052120a7bc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:24:36 minikube dockerd[1025]: time="2022-12-15T03:24:36.592366300Z" level=info msg="ignoring event" container=f720ac5d6df76e0bc6f53c871ca6835ea746c3f5b144d9fcbaa3631321e191f0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:28:03 minikube dockerd[1025]: time="2022-12-15T03:28:03.577528500Z" level=info msg="ignoring event" container=18f51c38c8f638b3b0481d3756833b261f7e8cfd300305921bb8ca22f5df0500 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:29:38 minikube dockerd[1025]: time="2022-12-15T03:29:38.364066600Z" level=info msg="ignoring event" container=7d14d2cd3871ad3ca2d7c014784fc3a45258ac2c99912d58393ec8c0b20b71a1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:33:18 minikube dockerd[1025]: time="2022-12-15T03:33:18.351045600Z" level=info msg="ignoring event" container=9947b3ece2d779ddffc0225a4efd40329c62fe06eb29cde05b66b7b39f2cf58b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:34:48 minikube dockerd[1025]: time="2022-12-15T03:34:48.943641100Z" level=info msg="ignoring event" container=aea9880181b6272f97a522cc04cd4664a436291599be6688c032674316f29527 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:38:25 minikube dockerd[1025]: time="2022-12-15T03:38:25.187636900Z" level=info msg="ignoring event" container=def101b8e53f995cec5183f8df0c6bf3bc745e06386eb5f91cb8a8c504293b52 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:39:51 minikube dockerd[1025]: time="2022-12-15T03:39:51.006296900Z" level=info msg="ignoring event" container=e30f1aacf683c06ab494bf9518cae90f3d863d6474b6b189b72d90e0fc232085 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:43:30 minikube dockerd[1025]: time="2022-12-15T03:43:30.581888500Z" level=info msg="ignoring event" container=ab0ef644f0fb1956f29fdf2ca66e791c52d8cf92ef302b12dba27d60b4fd9808 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:45:01 minikube dockerd[1025]: time="2022-12-15T03:45:01.983760800Z" level=info msg="ignoring event" container=3141a8678d2bbf0a7668a2274a2cbdd3c7ce3338f9e79a71067b21a353a3b47d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:48:35 minikube dockerd[1025]: time="2022-12-15T03:48:35.936051900Z" level=info msg="ignoring event" container=9b3c616a724829ac1198a2d0f6d2e88d60268b711b727a922d67b96c940df704 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:50:12 minikube dockerd[1025]: time="2022-12-15T03:50:12.078995500Z" level=info msg="ignoring event" container=771f76dbdee173ba601f2167e67d9049a3763fcfa74e6f542a0fd3895bde0120 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:51:38 minikube dockerd[1025]: time="2022-12-15T03:51:38.070441100Z" level=warning msg="reference for unknown type: " digest="sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f"
Dec 15 03:51:46 minikube dockerd[1025]: time="2022-12-15T03:51:46.829572500Z" level=info msg="ignoring event" container=6c9b0f0596252749eb448064b80f21c80a8d4656ff5a550c7008d8a11f7c0776 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:51:47 minikube dockerd[1025]: time="2022-12-15T03:51:47.043562000Z" level=info msg="ignoring event" container=43661ceeadfdee304daf669f709f635d609bac798c9270c59c32eaf6db1b91aa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:51:48 minikube dockerd[1025]: time="2022-12-15T03:51:48.503013400Z" level=info msg="ignoring event" container=1dc9bedaff839504b459016206408b7f9c5d188d9008f6f41a946138ea3d0c0c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:51:48 minikube dockerd[1025]: time="2022-12-15T03:51:48.518667700Z" level=info msg="ignoring event" container=97fe521bc1d6cbe1299ea5f5e7d61a47bb2f69862aa882e1b38a3f761e697a6a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:51:51 minikube dockerd[1025]: time="2022-12-15T03:51:51.330178400Z" level=warning msg="reference for unknown type: " digest="sha256:4ba73c697770664c1e00e9f968de14e08f606ff961c76e5d7033a4a9c593c629" remote="registry.k8s.io/ingress-nginx/controller@sha256:4ba73c697770664c1e00e9f968de14e08f606ff961c76e5d7033a4a9c593c629"
Dec 15 03:52:50 minikube dockerd[1025]: time="2022-12-15T03:52:50.337228800Z" level=warning msg="reference for unknown type: " digest="sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8" remote="k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Dec 15 03:53:00 minikube dockerd[1025]: time="2022-12-15T03:53:00.339426500Z" level=info msg="ignoring event" container=574a77cb2623c6c8ddfbc61abaaec6bc595a070bc7fdc785565d77cd54ef6423 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:53:00 minikube dockerd[1025]: time="2022-12-15T03:53:00.478195500Z" level=info msg="ignoring event" container=af3dd844489231b54468caa753de121be6d7b471e8a50b4233868f6bfb2ee7de module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:53:48 minikube dockerd[1025]: time="2022-12-15T03:53:48.186934400Z" level=info msg="ignoring event" container=8154aa94d4d816cc29e59fd23db5d9a7fc3525576bc02e56195d106dcb613281 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:55:13 minikube dockerd[1025]: time="2022-12-15T03:55:13.912388200Z" level=info msg="ignoring event" container=96ff365c92bc6457ba668bf65d3af7ffb2c4c6f959f593c16ba9c77acf43af7e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 03:59:00 minikube dockerd[1025]: time="2022-12-15T03:59:00.930815600Z" level=info msg="ignoring event" container=a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 04:00:17 minikube dockerd[1025]: time="2022-12-15T04:00:17.912307700Z" level=info msg="ignoring event" container=94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 04:04:07 minikube dockerd[1025]: time="2022-12-15T04:04:07.945268700Z" level=info msg="ignoring event" container=030d0b0344a6b33a3ddd5e6223bdfaa2a2214738107822f15d731b1c009c4ec0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID
030d0b0344a6b       postgres@sha256:10d6e725f9b2f5531617d93164f4fc85b1739e04cab62cbfbfb81ccd866513b8                                             26 seconds ago      Exited              postgres                  24                  ca172c5bc9cae
94b92b4331763       postgres@sha256:10d6e725f9b2f5531617d93164f4fc85b1739e04cab62cbfbfb81ccd866513b8                                             4 minutes ago       Exited              postgres                  18                  f621db3efc747
168c385d8f443       httpd@sha256:550274cfad1e6ab57cd5009b22a2d97946bdae50989db350a222dae93babc723                                                5 minutes ago       Running             httpd                     0                   ea5119c8a8925
aac01fb9aa7bf       k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8                  11 minutes ago      Running             controller                0                   b44189947705a
43661ceeadfde       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f   12 minutes ago      Exited              patch                     0                   97fe521bc1d6c
6c9b0f0596252       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f   12 minutes ago      Exited              create                    0                   1dc9bedaff839
fc1df996af810       stephengrider/multi-server@sha256:276e6416dba68f90bd1dc0d54fff19b3ac8d49ca8b4851dc9a2fa852a4092054                           About an hour ago   Running             server                    0                   afcab1b354e92
4bccaa7c0bbcb       stephengrider/multi-server@sha256:276e6416dba68f90bd1dc0d54fff19b3ac8d49ca8b4851dc9a2fa852a4092054                           About an hour ago   Running             server                    0                   4562fd606126d
d8e7a9f032433       stephengrider/multi-worker@sha256:5fbab5f86e6a4d499926349a5f0ec032c42e7f7450acc98b053791df26dc4d2b                           About an hour ago   Running             worker                    0                   54801c5cf29fe
bf4070c4f12e9       stephengrider/multi-server@sha256:276e6416dba68f90bd1dc0d54fff19b3ac8d49ca8b4851dc9a2fa852a4092054                           About an hour ago   Running             server                    0                   d2f5a767c30ef
f594df0abc566       6e38f40d628db                                                                                                                2 hours ago         Running             storage-provisioner       3                   2ca1a411c6785
8304076b37752       redis@sha256:4970a4bbd34f9072b56389e85185204dd07dc86ba74a1be441931d551f74b472                                                2 hours ago         Running             redis                     1                   58ba34806ed3d
61d7f03ad2f97       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                           2 hours ago         Running             client                    1                   4c04ec23992d2
e6b60c915ae47       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                           2 hours ago         Running             client                    1                   9fbb068b37c34
d9486241976dc       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                           2 hours ago         Running             client                    1                   b6d05601d819e
f7612e4ccf62a       5185b96f0becf                                                                                                                2 hours ago         Running             coredns                   1                   367ff27c333ed
028dbdf8c6d06       6e38f40d628db                                                                                                                2 hours ago         Exited              storage-provisioner       2                   2ca1a411c6785
556c7c5293595       beaaf00edd38a                                                                                                                2 hours ago         Running             kube-proxy                1                   2864ca6bea519
999edeed4c50b       6039992312758                                                                                                                2 hours ago         Running             kube-controller-manager   1                   5bbc36574e743
c1c6ba7a23376       6d23ec0e8b87e                                                                                                                2 hours ago         Running             kube-scheduler            1                   65d4afb1262b1
76d695526159b       0346dbd74bcb9                                                                                                                2 hours ago         Running             kube-apiserver            1                   562f3ae04bc89
c0c634d570de3       a8a176a5d5d69                                                                                                                2 hours ago         Running             etcd                      1                   a3e235d1d669d
9374fdd423757       redis@sha256:4970a4bbd34f9072b56389e85185204dd07dc86ba74a1be441931d551f74b472                                                34 hours ago        Exited              redis                     0                   33cb86a5ef5a2
52e8c0a5ce4b7       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                           35 hours ago        Exited              client                    0                   b143d87d65b8e
262aedd836980       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                           35 hours ago        Exited              client                    0                   f7dfd800e020d
f8c98740c3d27       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46                           35 hours ago        Exited              client                    0                   4d910ee498ef7
11259ef04cfcd       5185b96f0becf                                                                                                                2 days ago          Exited              coredns                   0                   1ee865686ea9e
d3880198668d1       beaaf00edd38a                                                                                                                2 days ago          Exited              kube-proxy                0                   a139154a1429f
26929076817d8       6039992312758                                                                                                                2 days ago          Exited              kube-controller-manager   0                   9d7893a6de449
7a2d092ce8de9       0346dbd74bcb9                                                                                                                2 days ago          Exited              kube-apiserver            0                   c1a62bf252cec
fd2082751d7c6       6d23ec0e8b87e                                                                                                                2 days ago          Exited              kube-scheduler            0                   8e1863f477fa1
1d1cab09d01db       a8a176a5d5d69                                                                                                                2 days ago          Exited              etcd                      0                   b0bf9ec3d89e7

* 
* ==> coredns [11259ef04cfc] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = a1b5920ef1e8e10875eeec3214b810e7e404fdaf6cfe53f31cc42ae1e9ba5884ecf886330489b6b02fba5b37a31406fcb402b2501c7ab0318fc890d74b6fae55
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11

* 
* ==> coredns [f7612e4ccf62] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = a1b5920ef1e8e10875eeec3214b810e7e404fdaf6cfe53f31cc42ae1e9ba5884ecf886330489b6b02fba5b37a31406fcb402b2501c7ab0318fc890d74b6fae55
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=986b1ebd987211ed16f8cc10aed7d2c42fc8392f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_12_12T22_01_42_0700
                    minikube.k8s.io/version=v1.28.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 13 Dec 2022 04:01:36 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Dec 2022 04:04:27 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 15 Dec 2022 04:04:10 +0000   Tue, 13 Dec 2022 04:01:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 15 Dec 2022 04:04:10 +0000   Tue, 13 Dec 2022 04:01:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 15 Dec 2022 04:04:10 +0000   Tue, 13 Dec 2022 04:01:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 15 Dec 2022 04:04:10 +0000   Tue, 13 Dec 2022 04:01:53 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  263174212Ki
  hugepages-2Mi:      0
  memory:             3700912Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  263174212Ki
  hugepages-2Mi:      0
  memory:             3700912Ki
  pods:               110
System Info:
  Machine ID:                 996614ec4c814b87b7ec8ebee3d0e8c9
  System UUID:                996614ec4c814b87b7ec8ebee3d0e8c9
  Boot ID:                    511417d1-0275-4e59-8f3a-c85c5c499033
  Kernel Version:             5.4.72-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.20
  Kubelet Version:            v1.25.3
  Kube-Proxy Version:         v1.25.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (19 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     client-deployment-576bc76988-g5nr4           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35h
  default                     client-deployment-576bc76988-jrxc6           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35h
  default                     client-deployment-576bc76988-kpgqx           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35h
  default                     demo-75ddddf99c-zj5g6                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m53s
  default                     postgres-deployment-5fbf95798b-bmqc8         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         98m
  default                     postgres-deployment-789465b8df-fv22k         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         71m
  default                     redis-deployment-6d5d95ddc4-sz7dt            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34h
  default                     server-deployment-74fc97747c-56kct           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69m
  default                     server-deployment-74fc97747c-5fvvs           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69m
  default                     server-deployment-74fc97747c-fgnwd           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69m
  default                     worker-deployment-64bfc78ff-44plk            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69m
  ingress-nginx               ingress-nginx-controller-5959f988fd-ptq5b    100m (0%!)(MISSING)     0 (0%!)(MISSING)      90Mi (2%!)(MISSING)        0 (0%!)(MISSING)         11m
  kube-system                 coredns-565d847f94-jhwc8                     100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     2d
  kube-system                 etcd-minikube                                100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         2d
  kube-system                 kube-apiserver-minikube                      250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d
  kube-system                 kube-controller-manager-minikube             200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d
  kube-system                 kube-proxy-pmkhg                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d
  kube-system                 kube-scheduler-minikube                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (7%!)(MISSING)   0 (0%!)(MISSING)
  memory             260Mi (7%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* 
* 
* ==> etcd [1d1cab09d01d] <==
* {"level":"info","ts":"2022-12-13T16:59:31.832Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7807}
{"level":"info","ts":"2022-12-13T16:59:31.834Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":7807,"took":"686.5¬µs"}
{"level":"info","ts":"2022-12-13T17:04:31.877Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8017}
{"level":"info","ts":"2022-12-13T17:04:31.879Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":8017,"took":"1.1086ms"}
{"level":"info","ts":"2022-12-13T17:09:31.888Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8228}
{"level":"info","ts":"2022-12-13T17:09:31.896Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":8228,"took":"6.3756ms"}
{"level":"info","ts":"2022-12-13T17:14:31.894Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8438}
{"level":"info","ts":"2022-12-13T17:14:31.896Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":8438,"took":"743.2¬µs"}
{"level":"info","ts":"2022-12-13T17:19:31.903Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8653}
{"level":"info","ts":"2022-12-13T17:19:31.904Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":8653,"took":"640¬µs"}
{"level":"info","ts":"2022-12-13T17:24:31.922Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8862}
{"level":"info","ts":"2022-12-13T17:24:31.923Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":8862,"took":"554¬µs"}
{"level":"info","ts":"2022-12-13T17:29:31.930Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9093}
{"level":"info","ts":"2022-12-13T17:29:31.932Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":9093,"took":"961.4¬µs"}
{"level":"info","ts":"2022-12-13T17:34:31.947Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9354}
{"level":"info","ts":"2022-12-13T17:34:31.952Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":9354,"took":"3.9386ms"}
{"level":"info","ts":"2022-12-13T17:39:31.957Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9564}
{"level":"info","ts":"2022-12-13T17:39:31.958Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":9564,"took":"753.2¬µs"}
{"level":"info","ts":"2022-12-13T17:44:31.977Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9776}
{"level":"info","ts":"2022-12-13T17:44:31.978Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":9776,"took":"681.6¬µs"}
{"level":"info","ts":"2022-12-13T17:49:31.995Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9991}
{"level":"info","ts":"2022-12-13T17:49:31.996Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":9991,"took":"728.1¬µs"}
{"level":"info","ts":"2022-12-13T17:54:32.004Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10224}
{"level":"info","ts":"2022-12-13T17:54:32.005Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":10224,"took":"785.8¬µs"}
{"level":"info","ts":"2022-12-13T17:59:32.011Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10435}
{"level":"info","ts":"2022-12-13T17:59:32.012Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":10435,"took":"724.6¬µs"}
{"level":"info","ts":"2022-12-13T18:04:32.029Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10696}
{"level":"info","ts":"2022-12-13T18:04:32.030Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":10696,"took":"801.8¬µs"}
{"level":"info","ts":"2022-12-13T18:09:32.035Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10912}
{"level":"info","ts":"2022-12-13T18:09:32.036Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":10912,"took":"702.4¬µs"}
{"level":"info","ts":"2022-12-13T18:14:32.042Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11126}
{"level":"info","ts":"2022-12-13T18:14:32.043Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":11126,"took":"581.7¬µs"}
{"level":"info","ts":"2022-12-13T18:19:32.060Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11340}
{"level":"info","ts":"2022-12-13T18:19:32.062Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":11340,"took":"906.1¬µs"}
{"level":"info","ts":"2022-12-13T18:24:32.068Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11554}
{"level":"info","ts":"2022-12-13T18:24:32.069Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":11554,"took":"640.7¬µs"}
{"level":"info","ts":"2022-12-13T18:29:32.086Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11787}
{"level":"info","ts":"2022-12-13T18:29:32.087Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":11787,"took":"866.5¬µs"}
{"level":"info","ts":"2022-12-13T18:34:32.094Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12044}
{"level":"info","ts":"2022-12-13T18:34:32.095Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":12044,"took":"722.8¬µs"}
{"level":"info","ts":"2022-12-13T18:39:32.109Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12258}
{"level":"info","ts":"2022-12-13T18:39:32.110Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":12258,"took":"554.5¬µs"}
{"level":"info","ts":"2022-12-13T18:44:32.118Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12471}
{"level":"info","ts":"2022-12-13T18:44:32.119Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":12471,"took":"715.3¬µs"}
{"level":"info","ts":"2022-12-13T18:49:32.127Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12685}
{"level":"info","ts":"2022-12-13T18:49:32.128Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":12685,"took":"562.4¬µs"}
{"level":"info","ts":"2022-12-13T18:54:32.133Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12899}
{"level":"info","ts":"2022-12-13T18:54:32.134Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":12899,"took":"883.7¬µs"}
{"level":"info","ts":"2022-12-13T18:59:32.141Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13114}
{"level":"info","ts":"2022-12-13T18:59:32.142Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13114,"took":"628.3¬µs"}
{"level":"info","ts":"2022-12-13T19:04:32.148Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13329}
{"level":"info","ts":"2022-12-13T19:04:32.149Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13329,"took":"473¬µs"}
{"level":"info","ts":"2022-12-13T19:09:32.156Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13542}
{"level":"info","ts":"2022-12-13T19:09:32.157Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13542,"took":"536¬µs"}
{"level":"info","ts":"2022-12-13T19:14:32.163Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13756}
{"level":"info","ts":"2022-12-13T19:14:32.164Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13756,"took":"747.5¬µs"}
{"level":"info","ts":"2022-12-13T19:19:32.170Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13968}
{"level":"info","ts":"2022-12-13T19:19:32.171Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13968,"took":"801¬µs"}
{"level":"info","ts":"2022-12-13T19:24:32.179Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14181}
{"level":"info","ts":"2022-12-13T19:24:32.181Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14181,"took":"709.2¬µs"}

* 
* ==> etcd [c0c634d570de] <==
* {"level":"info","ts":"2022-12-15T02:04:37.058Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15311}
{"level":"info","ts":"2022-12-15T02:04:37.059Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15311,"took":"637.7¬µs"}
{"level":"info","ts":"2022-12-15T02:08:29.003Z","caller":"etcdserver/server.go:1383","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2022-12-15T02:08:29.017Z","caller":"etcdserver/server.go:2394","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2022-12-15T02:08:29.018Z","caller":"etcdserver/server.go:2424","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2022-12-15T02:09:37.074Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15525}
{"level":"info","ts":"2022-12-15T02:09:37.075Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15525,"took":"618¬µs"}
{"level":"info","ts":"2022-12-15T02:14:37.081Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15737}
{"level":"info","ts":"2022-12-15T02:14:37.082Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15737,"took":"634.4¬µs"}
{"level":"info","ts":"2022-12-15T02:19:37.062Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15952}
{"level":"info","ts":"2022-12-15T02:19:37.063Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15952,"took":"705.5¬µs"}
{"level":"info","ts":"2022-12-15T02:24:37.038Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16164}
{"level":"info","ts":"2022-12-15T02:24:37.038Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16164,"took":"485¬µs"}
{"level":"info","ts":"2022-12-15T02:29:37.014Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16378}
{"level":"info","ts":"2022-12-15T02:29:37.015Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16378,"took":"453.8¬µs"}
{"level":"info","ts":"2022-12-15T02:34:36.994Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16675}
{"level":"info","ts":"2022-12-15T02:34:36.996Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16675,"took":"877¬µs"}
{"level":"info","ts":"2022-12-15T02:39:36.979Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16889}
{"level":"info","ts":"2022-12-15T02:39:36.982Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16889,"took":"2.5003ms"}
{"level":"info","ts":"2022-12-15T02:44:36.965Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17103}
{"level":"info","ts":"2022-12-15T02:44:36.966Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17103,"took":"655.7¬µs"}
{"level":"info","ts":"2022-12-15T02:49:36.950Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17316}
{"level":"info","ts":"2022-12-15T02:49:36.953Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17316,"took":"2.6488ms"}
{"level":"info","ts":"2022-12-15T02:54:36.941Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17591}
{"level":"info","ts":"2022-12-15T02:54:36.943Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17591,"took":"879.9¬µs"}
{"level":"warn","ts":"2022-12-15T02:54:52.015Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"184.4154ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/server-deployment-56685c959c\" ","response":"range_response_count:1 size:1977"}
{"level":"info","ts":"2022-12-15T02:54:52.020Z","caller":"traceutil/trace.go:171","msg":"trace[628640930] range","detail":"{range_begin:/registry/replicasets/default/server-deployment-56685c959c; range_end:; response_count:1; response_revision:17966; }","duration":"194.45ms","start":"2022-12-15T02:54:51.823Z","end":"2022-12-15T02:54:52.017Z","steps":["trace[628640930] 'agreement among raft nodes before linearized reading'  (duration: 181.6697ms)"],"step_count":1}
{"level":"info","ts":"2022-12-15T02:59:36.942Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17853}
{"level":"info","ts":"2022-12-15T02:59:36.943Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17853,"took":"673.4¬µs"}
{"level":"info","ts":"2022-12-15T03:04:36.944Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18210}
{"level":"info","ts":"2022-12-15T03:04:36.945Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18210,"took":"721.5¬µs"}
{"level":"info","ts":"2022-12-15T03:09:36.937Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18428}
{"level":"info","ts":"2022-12-15T03:09:36.938Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18428,"took":"616.4¬µs"}
{"level":"info","ts":"2022-12-15T03:14:36.933Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18645}
{"level":"info","ts":"2022-12-15T03:14:36.934Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18645,"took":"582.3¬µs"}
{"level":"info","ts":"2022-12-15T03:19:36.938Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18862}
{"level":"info","ts":"2022-12-15T03:19:36.939Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18862,"took":"588.2¬µs"}
{"level":"info","ts":"2022-12-15T03:24:36.941Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19077}
{"level":"info","ts":"2022-12-15T03:24:36.943Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19077,"took":"1.2699ms"}
{"level":"info","ts":"2022-12-15T03:29:36.939Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19293}
{"level":"info","ts":"2022-12-15T03:29:36.940Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19293,"took":"664.4¬µs"}
{"level":"info","ts":"2022-12-15T03:34:36.949Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19513}
{"level":"info","ts":"2022-12-15T03:34:36.951Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19513,"took":"1.0657ms"}
{"level":"warn","ts":"2022-12-15T03:39:26.015Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"203.5009ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2022-12-15T03:39:26.016Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"108.6943ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:135"}
{"level":"info","ts":"2022-12-15T03:39:26.231Z","caller":"traceutil/trace.go:171","msg":"trace[540647360] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:19936; }","duration":"797.852ms","start":"2022-12-15T03:39:25.320Z","end":"2022-12-15T03:39:26.118Z","steps":["trace[540647360] 'range keys from in-memory index tree'  (duration: 194.6801ms)"],"step_count":1}
{"level":"info","ts":"2022-12-15T03:39:26.224Z","caller":"traceutil/trace.go:171","msg":"trace[1657092288] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:19936; }","duration":"705.7523ms","start":"2022-12-15T03:39:25.416Z","end":"2022-12-15T03:39:26.121Z","steps":["trace[1657092288] 'range keys from in-memory index tree'  (duration: 100.3832ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-15T03:39:26.711Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-15T03:39:25.320Z","time spent":"1.2885522s","remote":"127.0.0.1:39632","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-12-15T03:39:26.709Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-15T03:39:25.415Z","time spent":"1.1950084s","remote":"127.0.0.1:39546","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":1,"response size":159,"request content":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" "}
{"level":"info","ts":"2022-12-15T03:39:36.948Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19729}
{"level":"info","ts":"2022-12-15T03:39:36.960Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19729,"took":"11.1962ms"}
{"level":"info","ts":"2022-12-15T03:44:36.962Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19946}
{"level":"info","ts":"2022-12-15T03:44:36.974Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19946,"took":"10.007ms"}
{"level":"info","ts":"2022-12-15T03:49:36.963Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20161}
{"level":"info","ts":"2022-12-15T03:49:36.972Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20161,"took":"7.9615ms"}
{"level":"info","ts":"2022-12-15T03:54:36.964Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20378}
{"level":"info","ts":"2022-12-15T03:54:36.993Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20378,"took":"27.5444ms"}
{"level":"info","ts":"2022-12-15T03:55:24.599Z","caller":"traceutil/trace.go:171","msg":"trace[1887114087] transaction","detail":"{read_only:false; response_revision:20831; number_of_response:1; }","duration":"102.4252ms","start":"2022-12-15T03:55:24.493Z","end":"2022-12-15T03:55:24.595Z","steps":["trace[1887114087] 'process raft request'  (duration: 96.4433ms)"],"step_count":1}
{"level":"info","ts":"2022-12-15T03:59:36.967Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20790}
{"level":"info","ts":"2022-12-15T03:59:36.985Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20790,"took":"17.2243ms"}

* 
* ==> kernel <==
*  04:04:33 up  2:38,  0 users,  load average: 0.36, 1.07, 1.38
Linux minikube 5.4.72-microsoft-standard-WSL2 #1 SMP Wed Oct 28 23:40:43 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [76d695526159] <==
* W1215 01:44:38.661253       1 genericapiserver.go:656] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1215 01:44:38.662189       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1215 01:44:38.662226       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W1215 01:44:38.704120       1 genericapiserver.go:656] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1215 01:44:41.059975       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1215 01:44:41.060146       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1215 01:44:41.060199       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1215 01:44:41.061214       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1215 01:44:41.061662       1 secure_serving.go:210] Serving securely on [::]:8443
I1215 01:44:41.062169       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1215 01:44:41.062429       1 available_controller.go:491] Starting AvailableConditionController
I1215 01:44:41.062491       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1215 01:44:41.062715       1 apf_controller.go:300] Starting API Priority and Fairness config controller
I1215 01:44:41.063053       1 controller.go:83] Starting OpenAPI AggregationController
I1215 01:44:41.064157       1 autoregister_controller.go:141] Starting autoregister controller
I1215 01:44:41.064240       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1215 01:44:41.064275       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1215 01:44:41.064609       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1215 01:44:41.064839       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1215 01:44:41.064882       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I1215 01:44:41.065085       1 controller.go:85] Starting OpenAPI controller
I1215 01:44:41.065211       1 controller.go:85] Starting OpenAPI V3 controller
I1215 01:44:41.065346       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1215 01:44:41.065388       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1215 01:44:41.066877       1 establishing_controller.go:76] Starting EstablishingController
I1215 01:44:41.066998       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1215 01:44:41.067095       1 crd_finalizer.go:266] Starting CRDFinalizer
I1215 01:44:41.066894       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1215 01:44:41.066904       1 naming_controller.go:291] Starting NamingConditionController
I1215 01:44:41.067386       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1215 01:44:41.067434       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I1215 01:44:41.068591       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1215 01:44:41.071504       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1215 01:44:41.207699       1 controller.go:616] quota admission added evaluator for: leases.coordination.k8s.io
I1215 01:44:41.295400       1 apf_controller.go:305] Running API Priority and Fairness config worker
I1215 01:44:41.295550       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I1215 01:44:41.295578       1 cache.go:39] Caches are synced for autoregister controller
I1215 01:44:41.295549       1 shared_informer.go:262] Caches are synced for crd-autoregister
I1215 01:44:41.295907       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1215 01:44:41.296405       1 shared_informer.go:262] Caches are synced for node_authorizer
I1215 01:44:41.296915       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
E1215 01:44:41.297106       1 controller.go:159] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I1215 01:44:41.704257       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1215 01:44:42.102458       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1215 01:44:46.703715       1 controller.go:616] quota admission added evaluator for: serviceaccounts
I1215 01:44:46.902742       1 controller.go:616] quota admission added evaluator for: deployments.apps
I1215 01:44:47.514094       1 controller.go:616] quota admission added evaluator for: daemonsets.apps
I1215 01:44:48.003013       1 controller.go:616] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1215 01:44:48.201707       1 controller.go:616] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1215 01:44:55.801452       1 controller.go:616] quota admission added evaluator for: endpoints
I1215 01:44:58.412748       1 controller.go:616] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1215 02:25:43.852265       1 controller.go:616] quota admission added evaluator for: replicasets.apps
I1215 03:39:26.819726       1 trace.go:205] Trace[219202966]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (15-Dec-2022 03:39:25.412) (total time: 1395ms):
Trace[219202966]: [1.395588s] [1.395588s] END
I1215 03:51:32.674570       1 controller.go:616] quota admission added evaluator for: namespaces
I1215 03:51:33.097854       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.99.144.238]
I1215 03:51:33.298969       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.96.231.224]
I1215 03:51:33.526162       1 controller.go:616] quota admission added evaluator for: jobs.batch
I1215 03:58:40.157998       1 alloc.go:327] "allocated clusterIPs" service="default/demo" clusterIPs=map[IPv4:10.101.220.63]
I1215 03:59:30.318089       1 controller.go:616] quota admission added evaluator for: ingresses.networking.k8s.io

* 
* ==> kube-apiserver [7a2d092ce8de] <==
* I1213 04:01:36.581133       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1213 04:01:36.581266       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1213 04:01:36.581121       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1213 04:01:36.581585       1 secure_serving.go:210] Serving securely on [::]:8443
I1213 04:01:36.581639       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1213 04:01:36.581781       1 apf_controller.go:300] Starting API Priority and Fairness config controller
I1213 04:01:36.581789       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1213 04:01:36.582004       1 available_controller.go:491] Starting AvailableConditionController
I1213 04:01:36.582028       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1213 04:01:36.582048       1 controller.go:83] Starting OpenAPI AggregationController
I1213 04:01:36.582076       1 autoregister_controller.go:141] Starting autoregister controller
I1213 04:01:36.582097       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1213 04:01:36.582364       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1213 04:01:36.582388       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I1213 04:01:36.582413       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1213 04:01:36.582439       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I1213 04:01:36.582515       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1213 04:01:36.582888       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1213 04:01:36.582974       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1213 04:01:36.583205       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1213 04:01:36.584074       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1213 04:01:36.585136       1 controller.go:85] Starting OpenAPI controller
I1213 04:01:36.585239       1 controller.go:85] Starting OpenAPI V3 controller
I1213 04:01:36.585306       1 naming_controller.go:291] Starting NamingConditionController
I1213 04:01:36.585413       1 establishing_controller.go:76] Starting EstablishingController
I1213 04:01:36.587074       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1213 04:01:36.587363       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1213 04:01:36.587410       1 crd_finalizer.go:266] Starting CRDFinalizer
I1213 04:01:36.665685       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1213 04:01:36.764270       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1213 04:01:36.772659       1 cache.go:39] Caches are synced for autoregister controller
I1213 04:01:36.782425       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1213 04:01:36.782830       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I1213 04:01:36.782900       1 apf_controller.go:305] Running API Priority and Fairness config worker
I1213 04:01:36.866371       1 shared_informer.go:262] Caches are synced for crd-autoregister
I1213 04:01:36.867605       1 shared_informer.go:262] Caches are synced for node_authorizer
I1213 04:01:36.871548       1 controller.go:616] quota admission added evaluator for: namespaces
I1213 04:01:37.164581       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1213 04:01:37.594450       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1213 04:01:37.674007       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1213 04:01:37.674127       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1213 04:01:39.784097       1 controller.go:616] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1213 04:01:39.977924       1 controller.go:616] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1213 04:01:40.301332       1 alloc.go:327] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W1213 04:01:40.316123       1 lease.go:250] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1213 04:01:40.318694       1 controller.go:616] quota admission added evaluator for: endpoints
I1213 04:01:40.329153       1 controller.go:616] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1213 04:01:40.914549       1 controller.go:616] quota admission added evaluator for: serviceaccounts
I1213 04:01:42.291292       1 controller.go:616] quota admission added evaluator for: deployments.apps
I1213 04:01:42.381801       1 alloc.go:327] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I1213 04:01:42.469680       1 controller.go:616] quota admission added evaluator for: daemonsets.apps
I1213 04:01:42.877168       1 controller.go:616] quota admission added evaluator for: leases.coordination.k8s.io
I1213 04:01:54.977876       1 controller.go:616] quota admission added evaluator for: replicasets.apps
I1213 04:01:54.981700       1 controller.go:616] quota admission added evaluator for: controllerrevisions.apps
E1213 15:09:58.284324       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1213 15:09:58.284355       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I1213 16:43:54.109996       1 alloc.go:327] "allocated clusterIPs" service="default/client-cluster-ip-service" clusterIPs=map[IPv4:10.100.224.13]
I1213 17:12:20.024799       1 alloc.go:327] "allocated clusterIPs" service="default/server-cluster-ip-service" clusterIPs=map[IPv4:10.109.141.125]
I1213 17:40:51.958158       1 alloc.go:327] "allocated clusterIPs" service="default/redis-cluster-ip-service" clusterIPs=map[IPv4:10.107.122.85]
I1213 17:57:12.802638       1 alloc.go:327] "allocated clusterIPs" service="default/postgres-cluster-ip-service" clusterIPs=map[IPv4:10.103.181.141]

* 
* ==> kube-controller-manager [26929076817d] <==
* I1213 04:01:54.576108       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I1213 04:01:54.578135       1 shared_informer.go:262] Caches are synced for ReplicaSet
I1213 04:01:54.582250       1 shared_informer.go:262] Caches are synced for endpoint_slice
I1213 04:01:54.582494       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1213 04:01:54.582620       1 shared_informer.go:262] Caches are synced for crt configmap
I1213 04:01:54.582645       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I1213 04:01:54.663181       1 shared_informer.go:262] Caches are synced for endpoint
I1213 04:01:54.663359       1 shared_informer.go:262] Caches are synced for PV protection
I1213 04:01:54.663398       1 shared_informer.go:262] Caches are synced for deployment
I1213 04:01:54.663467       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I1213 04:01:54.663505       1 shared_informer.go:262] Caches are synced for daemon sets
I1213 04:01:54.663813       1 range_allocator.go:367] Set node minikube PodCIDR to [10.244.0.0/24]
I1213 04:01:54.663987       1 shared_informer.go:262] Caches are synced for disruption
I1213 04:01:54.664210       1 shared_informer.go:262] Caches are synced for job
I1213 04:01:54.664463       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I1213 04:01:54.664703       1 shared_informer.go:262] Caches are synced for GC
I1213 04:01:54.666068       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I1213 04:01:54.668244       1 shared_informer.go:262] Caches are synced for ReplicationController
I1213 04:01:54.670366       1 shared_informer.go:262] Caches are synced for HPA
I1213 04:01:54.672346       1 shared_informer.go:262] Caches are synced for taint
I1213 04:01:54.673465       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
I1213 04:01:54.673597       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
W1213 04:01:54.673699       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1213 04:01:54.673783       1 node_lifecycle_controller.go:1259] Controller detected that zone  is now in state Normal.
I1213 04:01:54.673906       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1213 04:01:54.673705       1 taint_manager.go:209] "Sending events to api server"
I1213 04:01:54.675012       1 shared_informer.go:262] Caches are synced for attach detach
I1213 04:01:54.675842       1 shared_informer.go:262] Caches are synced for namespace
I1213 04:01:54.676199       1 shared_informer.go:262] Caches are synced for PVC protection
I1213 04:01:54.763144       1 shared_informer.go:262] Caches are synced for stateful set
I1213 04:01:54.763256       1 shared_informer.go:262] Caches are synced for persistent volume
I1213 04:01:54.763182       1 shared_informer.go:262] Caches are synced for ephemeral
I1213 04:01:54.763257       1 shared_informer.go:262] Caches are synced for expand
I1213 04:01:54.771931       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I1213 04:01:54.773135       1 shared_informer.go:262] Caches are synced for service account
I1213 04:01:54.863502       1 shared_informer.go:262] Caches are synced for resource quota
I1213 04:01:54.863678       1 shared_informer.go:262] Caches are synced for resource quota
I1213 04:01:55.072899       1 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-565d847f94 to 1"
I1213 04:01:55.166182       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-pmkhg"
I1213 04:01:55.173028       1 shared_informer.go:262] Caches are synced for garbage collector
I1213 04:01:55.174356       1 shared_informer.go:262] Caches are synced for garbage collector
I1213 04:01:55.174450       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1213 04:01:55.366954       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-565d847f94-jhwc8"
E1213 15:09:58.296177       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
W1213 15:09:58.295453       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
I1213 15:59:42.873737       1 cleaner.go:172] Cleaning CSR "csr-m4rvv" as it is more than 1h0m0s old and approved.
I1213 16:42:53.794893       1 event.go:294] "Event occurred" object="default/client-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set client-deployment-576bc76988 to 3"
I1213 16:42:53.877953       1 event.go:294] "Event occurred" object="default/client-deployment-576bc76988" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: client-deployment-576bc76988-g5nr4"
I1213 16:42:53.888606       1 event.go:294] "Event occurred" object="default/client-deployment-576bc76988" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: client-deployment-576bc76988-kpgqx"
I1213 16:42:53.890070       1 event.go:294] "Event occurred" object="default/client-deployment-576bc76988" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: client-deployment-576bc76988-jrxc6"
I1213 17:21:00.605205       1 event.go:294] "Event occurred" object="default/worker-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set worker-deployment-7c49d8f5f4 to 1"
I1213 17:21:00.682948       1 event.go:294] "Event occurred" object="default/worker-deployment-7c49d8f5f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: worker-deployment-7c49d8f5f4-7lx26"
I1213 17:25:24.474053       1 event.go:294] "Event occurred" object="default/server-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set server-deployment-56685c959c to 3"
I1213 17:25:24.492774       1 event.go:294] "Event occurred" object="default/server-deployment-56685c959c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-56685c959c-2rr4x"
I1213 17:25:24.503885       1 event.go:294] "Event occurred" object="default/server-deployment-56685c959c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-56685c959c-676tc"
I1213 17:25:24.503961       1 event.go:294] "Event occurred" object="default/server-deployment-56685c959c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-56685c959c-s6m5s"
I1213 17:47:44.389514       1 event.go:294] "Event occurred" object="default/redis-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set redis-deployment-6d5d95ddc4 to 1"
I1213 17:47:44.477561       1 event.go:294] "Event occurred" object="default/redis-deployment-6d5d95ddc4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: redis-deployment-6d5d95ddc4-sz7dt"
I1213 17:56:01.331117       1 event.go:294] "Event occurred" object="default/postgres-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres-deployment-df96cffb9 to 1"
I1213 17:56:01.348369       1 event.go:294] "Event occurred" object="default/postgres-deployment-df96cffb9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres-deployment-df96cffb9-r7jzz"

* 
* ==> kube-controller-manager [999edeed4c50] <==
* I1215 01:44:58.504832       1 shared_informer.go:262] Caches are synced for garbage collector
I1215 01:44:58.504926       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1215 01:44:58.511833       1 shared_informer.go:262] Caches are synced for garbage collector
I1215 01:48:12.568253       1 event.go:294] "Event occurred" object="default/postgres-deployment-df96cffb9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres-deployment-df96cffb9-xrh78"
I1215 02:25:43.779099       1 event.go:294] "Event occurred" object="default/database-persistent-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I1215 02:25:43.857962       1 event.go:294] "Event occurred" object="default/postgres-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres-deployment-5fbf95798b to 1"
I1215 02:25:43.866814       1 event.go:294] "Event occurred" object="default/postgres-deployment-5fbf95798b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres-deployment-5fbf95798b-bmqc8"
I1215 02:25:48.659188       1 event.go:294] "Event occurred" object="default/postgres-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set postgres-deployment-df96cffb9 to 0 from 1"
I1215 02:25:48.743877       1 event.go:294] "Event occurred" object="default/postgres-deployment-df96cffb9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: postgres-deployment-df96cffb9-xrh78"
I1215 02:52:44.007162       1 event.go:294] "Event occurred" object="default/postgres-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres-deployment-789465b8df to 1"
I1215 02:52:44.037591       1 event.go:294] "Event occurred" object="default/postgres-deployment-789465b8df" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres-deployment-789465b8df-fv22k"
I1215 02:54:44.784468       1 event.go:294] "Event occurred" object="default/server-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set server-deployment-74fc97747c to 1"
I1215 02:54:44.806973       1 event.go:294] "Event occurred" object="default/worker-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set worker-deployment-64bfc78ff to 1"
I1215 02:54:44.812645       1 event.go:294] "Event occurred" object="default/server-deployment-74fc97747c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-74fc97747c-56kct"
I1215 02:54:44.823515       1 event.go:294] "Event occurred" object="default/worker-deployment-64bfc78ff" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: worker-deployment-64bfc78ff-44plk"
I1215 02:54:47.032145       1 event.go:294] "Event occurred" object="default/server-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set server-deployment-56685c959c to 2 from 3"
I1215 02:54:47.104915       1 event.go:294] "Event occurred" object="default/server-deployment-56685c959c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: server-deployment-56685c959c-676tc"
I1215 02:54:47.111573       1 event.go:294] "Event occurred" object="default/server-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set server-deployment-74fc97747c to 2 from 1"
I1215 02:54:47.121223       1 event.go:294] "Event occurred" object="default/server-deployment-74fc97747c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-74fc97747c-5fvvs"
I1215 02:54:50.406360       1 event.go:294] "Event occurred" object="default/worker-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set worker-deployment-7c49d8f5f4 to 0 from 1"
I1215 02:54:50.425335       1 event.go:294] "Event occurred" object="default/worker-deployment-7c49d8f5f4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: worker-deployment-7c49d8f5f4-7lx26"
I1215 02:54:51.613070       1 event.go:294] "Event occurred" object="default/server-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set server-deployment-56685c959c to 1 from 2"
I1215 02:54:51.709809       1 event.go:294] "Event occurred" object="default/server-deployment-56685c959c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: server-deployment-56685c959c-2rr4x"
I1215 02:54:51.709875       1 event.go:294] "Event occurred" object="default/server-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set server-deployment-74fc97747c to 3 from 2"
I1215 02:54:51.717427       1 event.go:294] "Event occurred" object="default/server-deployment-74fc97747c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: server-deployment-74fc97747c-fgnwd"
I1215 02:54:54.830586       1 event.go:294] "Event occurred" object="default/server-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set server-deployment-56685c959c to 0 from 1"
I1215 02:54:54.904821       1 event.go:294] "Event occurred" object="default/server-deployment-56685c959c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: server-deployment-56685c959c-s6m5s"
I1215 03:51:33.598276       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:33.617495       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:33.797570       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:33.797501       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-8574b6d7c9 to 1"
I1215 03:51:33.804183       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:33.805194       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-9ss2c"
I1215 03:51:33.805358       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-lqww5"
I1215 03:51:33.910850       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:33.998876       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:33.999258       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-8574b6d7c9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-8574b6d7c9-tm4c2"
I1215 03:51:34.295052       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:34.298750       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:34.408896       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:34.422848       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:47.142069       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:47.152899       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:49.414444       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:49.499126       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:50.422526       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:50.433946       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1215 03:51:50.434029       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1215 03:51:50.442242       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
E1215 03:51:50.447842       1 job_controller.go:545] syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch "ingress-nginx-admission-patch": the object has been modified; please apply your changes to the latest version and try again
I1215 03:51:50.507375       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:50.514611       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:51:50.514997       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1215 03:51:50.520922       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1215 03:52:47.104813       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5959f988fd to 1"
I1215 03:52:47.195202       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5959f988fd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5959f988fd-ptq5b"
I1215 03:52:47.206006       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-8574b6d7c9 to 0 from 1"
I1215 03:52:47.395944       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-8574b6d7c9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-8574b6d7c9-tm4c2"
I1215 03:58:40.014131       1 event.go:294] "Event occurred" object="default/demo" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set demo-75ddddf99c to 1"
I1215 03:58:40.024592       1 event.go:294] "Event occurred" object="default/demo-75ddddf99c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: demo-75ddddf99c-zj5g6"

* 
* ==> kube-proxy [556c7c529359] <==
* E1215 01:44:51.410075       1 proxier.go:656] "Failed to read builtin modules file, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" err="open /lib/modules/5.4.72-microsoft-standard-WSL2/modules.builtin: no such file or directory" filePath="/lib/modules/5.4.72-microsoft-standard-WSL2/modules.builtin"
I1215 01:44:51.612053       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I1215 01:44:51.616005       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I1215 01:44:51.700581       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I1215 01:44:51.704124       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I1215 01:44:51.710419       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="nf_conntrack"
I1215 01:44:51.819936       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1215 01:44:51.820039       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1215 01:44:51.820128       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1215 01:44:52.400201       1 server_others.go:206] "Using iptables Proxier"
I1215 01:44:52.400285       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1215 01:44:52.400298       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1215 01:44:52.400327       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1215 01:44:52.403792       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1215 01:44:52.404188       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1215 01:44:52.405083       1 server.go:661] "Version info" version="v1.25.3"
I1215 01:44:52.405141       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1215 01:44:52.411454       1 config.go:226] "Starting endpoint slice config controller"
I1215 01:44:52.411427       1 config.go:317] "Starting service config controller"
I1215 01:44:52.411788       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1215 01:44:52.411864       1 shared_informer.go:255] Waiting for caches to sync for service config
I1215 01:44:52.411931       1 config.go:444] "Starting node config controller"
I1215 01:44:52.411943       1 shared_informer.go:255] Waiting for caches to sync for node config
I1215 01:44:52.512027       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1215 01:44:52.512068       1 shared_informer.go:262] Caches are synced for service config
I1215 01:44:52.512143       1 shared_informer.go:262] Caches are synced for node config
E1215 03:52:47.407550       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:31465: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"

* 
* ==> kube-proxy [d3880198668d] <==
* E1213 04:01:57.690600       1 proxier.go:656] "Failed to read builtin modules file, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" err="open /lib/modules/5.4.72-microsoft-standard-WSL2/modules.builtin: no such file or directory" filePath="/lib/modules/5.4.72-microsoft-standard-WSL2/modules.builtin"
I1213 04:01:57.794486       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I1213 04:01:57.799764       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I1213 04:01:57.808261       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I1213 04:01:57.812217       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I1213 04:01:57.815711       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="nf_conntrack"
I1213 04:01:57.912770       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1213 04:01:57.912922       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1213 04:01:57.913016       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1213 04:01:57.994673       1 server_others.go:206] "Using iptables Proxier"
I1213 04:01:57.994829       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1213 04:01:57.994852       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1213 04:01:57.994922       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1213 04:01:57.997554       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1213 04:01:57.998137       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1213 04:01:58.005587       1 server.go:661] "Version info" version="v1.25.3"
I1213 04:01:58.005685       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1213 04:01:58.018542       1 config.go:226] "Starting endpoint slice config controller"
I1213 04:01:58.018727       1 config.go:444] "Starting node config controller"
I1213 04:01:58.019157       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1213 04:01:58.018588       1 config.go:317] "Starting service config controller"
I1213 04:01:58.019417       1 shared_informer.go:255] Waiting for caches to sync for service config
I1213 04:01:58.019211       1 shared_informer.go:255] Waiting for caches to sync for node config
I1213 04:01:58.119613       1 shared_informer.go:262] Caches are synced for service config
I1213 04:01:58.119672       1 shared_informer.go:262] Caches are synced for node config
I1213 04:01:58.121025       1 shared_informer.go:262] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [c1c6ba7a2337] <==
* I1215 01:44:36.126127       1 serving.go:348] Generated self-signed cert in-memory
W1215 01:44:41.201908       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1215 01:44:41.202024       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1215 01:44:41.202045       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W1215 01:44:41.202054       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1215 01:44:41.318141       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.3"
I1215 01:44:41.318191       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1215 01:44:41.398192       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1215 01:44:41.398782       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1215 01:44:41.398928       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1215 01:44:41.400283       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1215 01:44:41.500836       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1215 03:51:34.108177       1 trace.go:205] Trace[1162446864]: "Scheduling" namespace:ingress-nginx,name:ingress-nginx-admission-patch-9ss2c (15-Dec-2022 03:51:33.812) (total time: 283ms):
Trace[1162446864]: ---"Snapshotting scheduler cache and node infos done" 81ms (03:51:33.893)
Trace[1162446864]: ---"Computing predicates done" 201ms (03:51:34.095)
Trace[1162446864]: [283.6785ms] [283.6785ms] END

* 
* ==> kube-scheduler [fd2082751d7c] <==
* W1213 04:01:36.971065       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1213 04:01:36.971283       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1213 04:01:36.971288       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1213 04:01:36.971164       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1213 04:01:36.971570       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1213 04:01:36.971562       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1213 04:01:36.971691       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1213 04:01:36.971698       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1213 04:01:36.971765       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1213 04:01:36.971814       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1213 04:01:36.971870       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1213 04:01:36.971903       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1213 04:01:36.971965       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1213 04:01:36.972097       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1213 04:01:36.972139       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1213 04:01:36.972212       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1213 04:01:36.972257       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1213 04:01:36.972343       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1213 04:01:36.972406       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1213 04:01:36.972317       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1213 04:01:36.972469       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1213 04:01:36.972559       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1213 04:01:36.972586       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1213 04:01:36.972615       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1213 04:01:36.972616       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1213 04:01:36.974498       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1213 04:01:36.974624       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1213 04:01:36.976626       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1213 04:01:36.976737       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1213 04:01:37.798832       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1213 04:01:37.798948       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1213 04:01:37.888038       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1213 04:01:37.888166       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1213 04:01:37.996886       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1213 04:01:37.996987       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1213 04:01:38.023701       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1213 04:01:38.023820       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1213 04:01:38.069587       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1213 04:01:38.069745       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1213 04:01:38.109991       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1213 04:01:38.110091       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1213 04:01:38.179554       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1213 04:01:38.179666       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1213 04:01:38.308182       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1213 04:01:38.308295       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1213 04:01:38.327032       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1213 04:01:38.327146       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1213 04:01:38.366544       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1213 04:01:38.366603       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1213 04:01:38.366641       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1213 04:01:38.366654       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1213 04:01:38.366842       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1213 04:01:38.366925       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1213 04:01:38.369117       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1213 04:01:38.369224       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1213 04:01:38.394660       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1213 04:01:38.394747       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1213 04:01:38.466545       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1213 04:01:38.466698       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
I1213 04:01:41.082006       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Thu 2022-12-15 01:44:16 UTC, end at Thu 2022-12-15 04:04:33 UTC. --
Dec 15 04:01:24 minikube kubelet[1619]: E1215 04:01:24.892939    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:01:30 minikube kubelet[1619]: I1215 04:01:30.892728    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:01:30 minikube kubelet[1619]: E1215 04:01:30.893233    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:01:39 minikube kubelet[1619]: I1215 04:01:39.892944    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:01:39 minikube kubelet[1619]: E1215 04:01:39.893170    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:01:41 minikube kubelet[1619]: I1215 04:01:41.892835    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:01:41 minikube kubelet[1619]: E1215 04:01:41.893075    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:01:53 minikube kubelet[1619]: I1215 04:01:53.892410    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:01:53 minikube kubelet[1619]: E1215 04:01:53.892763    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:01:56 minikube kubelet[1619]: I1215 04:01:56.891558    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:01:56 minikube kubelet[1619]: E1215 04:01:56.891806    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:02:08 minikube kubelet[1619]: I1215 04:02:08.891632    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:02:08 minikube kubelet[1619]: E1215 04:02:08.891894    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:02:10 minikube kubelet[1619]: I1215 04:02:10.892007    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:02:10 minikube kubelet[1619]: E1215 04:02:10.892248    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:02:20 minikube kubelet[1619]: I1215 04:02:20.891866    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:02:20 minikube kubelet[1619]: E1215 04:02:20.892111    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:02:25 minikube kubelet[1619]: I1215 04:02:25.891585    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:02:25 minikube kubelet[1619]: E1215 04:02:25.891991    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:02:33 minikube kubelet[1619]: I1215 04:02:33.892247    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:02:33 minikube kubelet[1619]: E1215 04:02:33.892507    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:02:37 minikube kubelet[1619]: I1215 04:02:37.891890    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:02:37 minikube kubelet[1619]: E1215 04:02:37.892237    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:02:47 minikube kubelet[1619]: I1215 04:02:47.891705    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:02:47 minikube kubelet[1619]: E1215 04:02:47.892199    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:02:49 minikube kubelet[1619]: I1215 04:02:49.891838    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:02:49 minikube kubelet[1619]: E1215 04:02:49.892274    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:03:01 minikube kubelet[1619]: I1215 04:03:01.892370    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:03:01 minikube kubelet[1619]: I1215 04:03:01.892502    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:03:01 minikube kubelet[1619]: E1215 04:03:01.892632    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:03:01 minikube kubelet[1619]: E1215 04:03:01.892647    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:03:12 minikube kubelet[1619]: I1215 04:03:12.891610    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:03:12 minikube kubelet[1619]: E1215 04:03:12.892067    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:03:16 minikube kubelet[1619]: I1215 04:03:16.891060    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:03:16 minikube kubelet[1619]: E1215 04:03:16.891302    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:03:25 minikube kubelet[1619]: I1215 04:03:25.891080    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:03:25 minikube kubelet[1619]: E1215 04:03:25.891315    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:03:27 minikube kubelet[1619]: I1215 04:03:27.891112    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:03:27 minikube kubelet[1619]: E1215 04:03:27.891348    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:03:39 minikube kubelet[1619]: I1215 04:03:39.891602    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:03:39 minikube kubelet[1619]: E1215 04:03:39.891889    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:03:42 minikube kubelet[1619]: I1215 04:03:42.890748    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:03:42 minikube kubelet[1619]: E1215 04:03:42.890991    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:03:53 minikube kubelet[1619]: I1215 04:03:53.890069    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:03:53 minikube kubelet[1619]: E1215 04:03:53.890617    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:03:54 minikube kubelet[1619]: I1215 04:03:54.890298    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:03:54 minikube kubelet[1619]: E1215 04:03:54.890632    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:04:04 minikube kubelet[1619]: I1215 04:04:04.890934    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:04:04 minikube kubelet[1619]: E1215 04:04:04.891301    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:04:06 minikube kubelet[1619]: I1215 04:04:06.891044    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:04:08 minikube kubelet[1619]: I1215 04:04:08.760721    1619 scope.go:115] "RemoveContainer" containerID="a0744166b5afd97592493bd7f100f63ecddf312b1ace0647f02266441aff1cc2"
Dec 15 04:04:08 minikube kubelet[1619]: I1215 04:04:08.761155    1619 scope.go:115] "RemoveContainer" containerID="030d0b0344a6b33a3ddd5e6223bdfaa2a2214738107822f15d731b1c009c4ec0"
Dec 15 04:04:08 minikube kubelet[1619]: E1215 04:04:08.761444    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:04:17 minikube kubelet[1619]: I1215 04:04:17.890315    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:04:17 minikube kubelet[1619]: E1215 04:04:17.890605    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61
Dec 15 04:04:23 minikube kubelet[1619]: I1215 04:04:23.890329    1619 scope.go:115] "RemoveContainer" containerID="030d0b0344a6b33a3ddd5e6223bdfaa2a2214738107822f15d731b1c009c4ec0"
Dec 15 04:04:23 minikube kubelet[1619]: E1215 04:04:23.890651    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-5fbf95798b-bmqc8_default(2d3c4d42-ae9a-4dec-bde6-2d7b47999627)\"" pod="default/postgres-deployment-5fbf95798b-bmqc8" podUID=2d3c4d42-ae9a-4dec-bde6-2d7b47999627
Dec 15 04:04:31 minikube kubelet[1619]: W1215 04:04:31.912706    1619 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 15 04:04:32 minikube kubelet[1619]: I1215 04:04:32.889914    1619 scope.go:115] "RemoveContainer" containerID="94b92b4331763a9fb9e0018227331f32c71cc2ee9665830cce19af3e3e7737f4"
Dec 15 04:04:32 minikube kubelet[1619]: E1215 04:04:32.890302    1619 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=postgres pod=postgres-deployment-789465b8df-fv22k_default(3f15216e-99f0-4577-836f-b0eabf568d61)\"" pod="default/postgres-deployment-789465b8df-fv22k" podUID=3f15216e-99f0-4577-836f-b0eabf568d61

* 
* ==> storage-provisioner [028dbdf8c6d0] <==
* I1215 01:44:51.515950       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1215 01:44:51.706593       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

* 
* ==> storage-provisioner [f594df0abc56] <==
* I1215 01:45:12.406039       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1215 01:45:12.420321       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1215 01:45:12.420809       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1215 01:45:29.821832       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1215 01:45:29.821946       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"de23fbbd-b036-45dc-893c-1025fa474591", APIVersion:"v1", ResourceVersion:"14630", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_06925701-8eb2-4b36-81d6-e5640f99d02d became leader
I1215 01:45:29.822038       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_06925701-8eb2-4b36-81d6-e5640f99d02d!
I1215 01:45:29.923988       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_06925701-8eb2-4b36-81d6-e5640f99d02d!
I1215 02:25:43.777485       1 controller.go:1332] provision "default/database-persistent-volume-claim" class "standard": started
I1215 02:25:43.785429       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"database-persistent-volume-claim", UID:"3ba5e50c-5a72-4868-b262-7389e54b03de", APIVersion:"v1", ResourceVersion:"16428", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/database-persistent-volume-claim"
I1215 02:25:43.782391       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    03a07121-3dff-482a-b6f0-7e8a379824b9 298 0 2022-12-13 04:01:50 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2022-12-13 04:01:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-3ba5e50c-5a72-4868-b262-7389e54b03de &PersistentVolumeClaim{ObjectMeta:{database-persistent-volume-claim  default  3ba5e50c-5a72-4868-b262-7389e54b03de 16428 0 2022-12-15 02:25:43 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"database-persistent-volume-claim","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"2Gi"}}}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2022-12-15 02:25:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2022-12-15 02:25:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/database-persistent-volume-claim
I1215 02:25:43.848559       1 controller.go:1439] provision "default/database-persistent-volume-claim" class "standard": volume "pvc-3ba5e50c-5a72-4868-b262-7389e54b03de" provisioned
I1215 02:25:43.848711       1 controller.go:1456] provision "default/database-persistent-volume-claim" class "standard": succeeded
I1215 02:25:43.848795       1 volume_store.go:212] Trying to save persistentvolume "pvc-3ba5e50c-5a72-4868-b262-7389e54b03de"
I1215 02:25:43.873273       1 volume_store.go:219] persistentvolume "pvc-3ba5e50c-5a72-4868-b262-7389e54b03de" saved
I1215 02:25:43.936152       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"database-persistent-volume-claim", UID:"3ba5e50c-5a72-4868-b262-7389e54b03de", APIVersion:"v1", ResourceVersion:"16428", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-3ba5e50c-5a72-4868-b262-7389e54b03de

